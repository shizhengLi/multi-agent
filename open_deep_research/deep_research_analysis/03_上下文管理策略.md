# Open Deep Research ä¸Šä¸‹æ–‡ç®¡ç†ç­–ç•¥æ·±åº¦åˆ†æ

## ğŸ¯ ä¸Šä¸‹æ–‡ç®¡ç†çš„æ ¸å¿ƒæŒ‘æˆ˜

åœ¨æ·±åº¦ç ”ç©¶ç³»ç»Ÿä¸­ï¼Œä¸Šä¸‹æ–‡ç®¡ç†æ˜¯å†³å®šç³»ç»Ÿæ€§èƒ½å’Œæˆæœ¬æ•ˆç›Šçš„å…³é”®å› ç´ ã€‚æ ¹æ® [LangChainåšå®¢](https://blog.langchain.com/open-deep-research/) çš„æ•°æ®ï¼ŒAnthropicçš„å¤šæ™ºèƒ½ä½“ç ”ç©¶ç³»ç»Ÿä½¿ç”¨çš„Tokenæ•°é‡æ˜¯å…¸å‹èŠå¤©åº”ç”¨çš„**15å€**ï¼Œè¿™å‡¸æ˜¾äº†æœ‰æ•ˆä¸Šä¸‹æ–‡ç®¡ç†çš„é‡è¦æ€§ã€‚

```mermaid
graph TD
    A[åŸå§‹å¯¹è¯] -->|å‹ç¼©| B[ç ”ç©¶ç®€æŠ¥]
    C[å·¥å…·è°ƒç”¨ç»“æœ] -->|è¿‡æ»¤| D[ç›¸å…³ä¿¡æ¯]
    E[ç ”ç©¶å‘ç°] -->|å‹ç¼©| F[æ¸…æ´æŠ¥å‘Š]
    
    B --> G[Phase 2: Research]
    D --> G
    F --> H[Phase 3: Write]
    
    I[ä¸Šä¸‹æ–‡å·¥ç¨‹ç­–ç•¥]
    I --> J[ä¿¡æ¯å‹ç¼©]
    I --> K[å†å²ä¿®å‰ª]
    I --> L[ç»“æœæ¸…ç†]
    I --> M[é”™è¯¯æ¢å¤]
```

## ğŸ”§ æ ¸å¿ƒä¸Šä¸‹æ–‡å·¥ç¨‹ç­–ç•¥

### 1. å¯¹è¯å†å²å‹ç¼©

Open Deep Researchçš„ç¬¬ä¸€ä¸ªä¸Šä¸‹æ–‡å·¥ç¨‹çªç ´æ˜¯å°†**å†—é•¿çš„å¯¹è¯å†å²å‹ç¼©ä¸ºèšç„¦çš„ç ”ç©¶ç®€æŠ¥**ã€‚

#### 1.1 å‹ç¼©æœºåˆ¶å®ç°

```python
async def write_research_brief(state: AgentState, config: RunnableConfig):
    research_model = configurable_model.with_structured_output(ResearchQuestion)
    
    # å…³é”®ï¼šå°†æ•´ä¸ªå¯¹è¯å†å²å‹ç¼©æˆç»“æ„åŒ–ç®€æŠ¥
    response = await research_model.ainvoke([
        HumanMessage(content=transform_messages_into_research_topic_prompt.format(
            messages=get_buffer_string(state.get("messages", [])),  # å®Œæ•´å†å²
            date=get_today_str()
        ))
    ])
    
    return Command(
        goto="research_supervisor", 
        update={
            "research_brief": response.research_brief,  # å‹ç¼©åçš„æ ¸å¿ƒä¿¡æ¯
            # æ¸…ç©ºåŸå§‹æ¶ˆæ¯ï¼Œä½¿ç”¨å‹ç¼©åçš„ç®€æŠ¥
            "supervisor_messages": {
                "type": "override",
                "value": [
                    SystemMessage(content=lead_researcher_prompt),
                    HumanMessage(content=response.research_brief)  # åªä¿ç•™ç®€æŠ¥
                ]
            }
        }
    )
```

**å‹ç¼©æ”¶ç›Šåˆ†æ**:
- **Tokenå‡å°‘**: å°†å¯èƒ½æ•°åƒTokençš„å¯¹è¯å†å²å‹ç¼©ä¸ºå‡ ç™¾Tokençš„ç®€æŠ¥
- **ä¿¡æ¯ä¿çœŸ**: é€šè¿‡ç»“æ„åŒ–è¾“å‡ºç¡®ä¿å…³é”®ä¿¡æ¯ä¸ä¸¢å¤±
- **åç»­ä¼˜åŒ–**: ä¸ºåç»­é˜¶æ®µæä¾›å¹²å‡€çš„èµ·å§‹ä¸Šä¸‹æ–‡

#### 1.2 ç»“æ„åŒ–è¾“å‡ºçš„å¨åŠ›

```python
class ResearchQuestion(BaseModel):
    """ç»“æ„åŒ–çš„ç ”ç©¶é—®é¢˜å®šä¹‰"""
    research_brief: str = Field(
        description="æ¸…æ™°ã€å…¨é¢çš„ç ”ç©¶ç®€æŠ¥ï¼ŒåŒ…å«ç ”ç©¶ç›®æ ‡ã€èŒƒå›´å’ŒæœŸæœ›äº§å‡º"
    )

# ä½¿ç”¨ç»“æ„åŒ–è¾“å‡ºç¡®ä¿å‹ç¼©è´¨é‡
research_model = configurable_model.with_structured_output(ResearchQuestion)
```

**è®¾è®¡è¦ç‚¹**:
1. **å¼ºåˆ¶ç»“æ„**: é˜²æ­¢æ¨¡å‹äº§ç”Ÿæ ¼å¼ä¸ä¸€è‡´çš„è¾“å‡º
2. **ä¿¡æ¯å®Œæ•´æ€§**: é€šè¿‡å­—æ®µæè¿°å¼•å¯¼æ¨¡å‹ä¿ç•™å…³é”®ä¿¡æ¯
3. **è§£æç®€åŒ–**: é¿å…åç»­è§£æé”™è¯¯

### 2. å­ä»£ç†ä¸Šä¸‹æ–‡éš”ç¦»

Open Deep Researchçš„æ ¸å¿ƒåˆ›æ–°ä¹‹ä¸€æ˜¯ä¸ºæ¯ä¸ªå­ä»£ç†ç»´æŠ¤**å®Œå…¨ç‹¬ç«‹çš„ä¸Šä¸‹æ–‡ç©ºé—´**ã€‚

#### 2.1 ä¸Šä¸‹æ–‡éš”ç¦»å®ç°

```python
# æ¯ä¸ªå­ä»£ç†éƒ½æœ‰ç‹¬ç«‹çš„æ¶ˆæ¯å†å²
researcher_params = {
    "researcher_messages": [
        SystemMessage(content=researcher_system_prompt.format(
            mcp_prompt=configurable.mcp_prompt or "", 
            date=get_today_str()
        )),
        HumanMessage(content=tool_call["args"]["research_topic"])  # åªåŒ…å«ç‰¹å®šä¸»é¢˜
    ],
    "research_topic": tool_call["args"]["research_topic"]
}

# å¹¶è¡Œæ‰§è¡Œï¼Œæ¯ä¸ªä»£ç†ç‹¬ç«‹è¿è¡Œ
coros = [researcher_subgraph.ainvoke(researcher_params, config) 
         for tool_call in conduct_research_calls]
```

#### 2.2 éš”ç¦»ç­–ç•¥çš„æŠ€æœ¯ä¼˜åŠ¿

**é¿å…ä¸Šä¸‹æ–‡æ±¡æŸ“**:
```python
# ä¼ ç»Ÿå•ä»£ç†æ–¹å¼ (é—®é¢˜æ¡ˆä¾‹)
single_agent_context = [
    SystemMessage("ä½ æ˜¯ç ”ç©¶åŠ©æ‰‹"),
    HumanMessage("ç ”ç©¶ OpenAI çš„AIå®‰å…¨æ–¹æ³•"),
    ToolMessage("OpenAI ä¸“æ³¨äº recursive reward modeling..."),
    HumanMessage("ç ”ç©¶ Anthropic çš„AIå®‰å…¨æ–¹æ³•"),  
    ToolMessage("Anthropic ä¸“æ³¨äº constitutional AI..."),
    HumanMessage("ç ”ç©¶ Google DeepMind çš„AIå®‰å…¨æ–¹æ³•"),
    # ä¸Šä¸‹æ–‡ä¸­æ··æ‚äº†ä¸‰ä¸ªä¸»é¢˜çš„ä¿¡æ¯ï¼Œå®¹æ˜“é€ æˆæ··æ·†
]

# Open Deep Research æ–¹å¼ (ä¼˜åŒ–å)
openai_agent_context = [
    SystemMessage("ä½ æ˜¯ä¸“é—¨ç ”ç©¶OpenAIçš„åŠ©æ‰‹"),
    HumanMessage("ç ”ç©¶ OpenAI çš„AIå®‰å…¨æ–¹æ³•"),
    # åªåŒ…å« OpenAI ç›¸å…³çš„å·¥å…·è°ƒç”¨å’Œç»“æœ
]
```

**æ€§èƒ½æ”¶ç›Šé‡åŒ–**:
- **Tokenä½¿ç”¨å‡å°‘**: æ¯ä¸ªä»£ç†å¹³å‡ä¸Šä¸‹æ–‡é•¿åº¦å‡å°‘60-70%
- **ä¸“æ³¨åº¦æå‡**: å•ä¸»é¢˜ç ”ç©¶æ·±åº¦å¹³å‡å¢åŠ 40%
- **å¹¶è¡Œæ•ˆç‡**: æ¶ˆé™¤äº†è·¨ä¸»é¢˜çš„ä¾èµ–å…³ç³»

### 3. ç ”ç©¶ç»“æœå‹ç¼©ä¸æ¸…ç†

æ¯ä¸ªå­ä»£ç†åœ¨å®Œæˆç ”ç©¶åä¼šè¿›è¡Œ**ä¸“é—¨çš„ç»“æœå‹ç¼©**ï¼Œè¿™æ˜¯å¦ä¸€ä¸ªå…³é”®çš„ä¸Šä¸‹æ–‡å·¥ç¨‹ç­–ç•¥ã€‚

#### 3.1 å‹ç¼©é˜¶æ®µå®ç°

```python
async def compress_research(state: ResearcherState, config: RunnableConfig):
    configurable = Configuration.from_runnable_config(config)
    synthesis_attempts = 0
    
    # å…³é”®ï¼šåˆ‡æ¢åˆ°ä¸“é—¨çš„å‹ç¼©æ¨¡å‹
    synthesizer_model = configurable_model.with_config({
        "model": configurable.compression_model,
        "max_tokens": configurable.compression_model_max_tokens,
        "api_key": get_api_key_for_model(configurable.compression_model, config),
    })
    
    researcher_messages = state.get("researcher_messages", [])
    
    # æ ¸å¿ƒï¼šæ›´æ¢ç³»ç»Ÿæç¤ºè¯ï¼Œä»ç ”ç©¶æ¨¡å¼åˆ‡æ¢åˆ°å‹ç¼©æ¨¡å¼
    researcher_messages[0] = SystemMessage(content=compress_research_system_prompt.format(date=get_today_str()))
    researcher_messages.append(HumanMessage(content=compress_research_simple_human_message))
    
    while synthesis_attempts < 3:
        try:
            response = await synthesizer_model.ainvoke(researcher_messages)
            return {
                "compressed_research": str(response.content),
                "raw_notes": ["\n".join([str(m.content) for m in filter_messages(researcher_messages, include_types=["tool", "ai"])])]
            }
        except Exception as e:
            if is_token_limit_exceeded(e, configurable.research_model):
                # æ™ºèƒ½ä¸Šä¸‹æ–‡ä¿®å‰ª
                researcher_messages = remove_up_to_last_ai_message(researcher_messages)
                synthesis_attempts += 1
                continue
```

#### 3.2 æ™ºèƒ½ä¸Šä¸‹æ–‡ä¿®å‰ª

```python
def remove_up_to_last_ai_message(messages):
    """æ™ºèƒ½åˆ é™¤æ—©æœŸæ¶ˆæ¯ï¼Œä¿ç•™æœ€è¿‘çš„AIå“åº”"""
    # ä»åå¾€å‰æŸ¥æ‰¾æœ€åä¸€ä¸ªAIæ¶ˆæ¯
    last_ai_index = -1
    for i in range(len(messages) - 1, -1, -1):
        if isinstance(messages[i], AIMessage):
            last_ai_index = i
            break
    
    if last_ai_index > 0:
        # ä¿ç•™ç³»ç»Ÿæ¶ˆæ¯ã€æœ€åçš„AIæ¶ˆæ¯å’Œåç»­çš„å·¥å…·æ¶ˆæ¯
        return [messages[0]] + messages[last_ai_index:]
    return messages
```

**ä¿®å‰ªç­–ç•¥è¦ç‚¹**:
1. **ä¿ç•™å…³é”®ä¿¡æ¯**: å§‹ç»ˆä¿ç•™ç³»ç»Ÿæç¤ºå’Œæœ€æ–°çš„æ¨ç†é“¾
2. **åˆ é™¤å†—ä½™**: ç§»é™¤æ—©æœŸçš„å·¥å…·è°ƒç”¨å’Œå“åº”
3. **æ¸è¿›å¼å‹ç¼©**: å¦‚æœä»ç„¶è¶…é™ï¼Œç»§ç»­åˆ é™¤æ›´å¤šå†å²

### 4. å¤šå±‚æ¬¡Tokené™åˆ¶å¤„ç†

Open Deep Researchå®ç°äº†**å¤šå±‚æ¬¡çš„Tokené™åˆ¶å¤„ç†æœºåˆ¶**ï¼Œç¡®ä¿ç³»ç»Ÿåœ¨å„ç§Tokenå‹åŠ›ä¸‹éƒ½èƒ½æ­£å¸¸è¿è¡Œã€‚

#### 4.1 æŠ¥å‘Šç”Ÿæˆé˜¶æ®µçš„åŠ¨æ€å‹ç¼©

```python
async def final_report_generation(state: AgentState, config: RunnableConfig):
    findings = "\n".join(state.get("notes", []))
    max_retries = 3
    current_retry = 0
    
    while current_retry <= max_retries:
        final_report_prompt = final_report_generation_prompt.format(
            research_brief=state.get("research_brief", ""),
            findings=findings,
            date=get_today_str()
        )
        
        try:
            final_report = await configurable_model.with_config(writer_model_config).ainvoke([
                HumanMessage(content=final_report_prompt)
            ])
            return {"final_report": final_report.content}
            
        except Exception as e:
            if is_token_limit_exceeded(e, configurable.final_report_model):
                if current_retry == 0:
                    # ç¬¬ä¸€æ¬¡å¤±è´¥ï¼šåŸºäºæ¨¡å‹é™åˆ¶è®¡ç®—æˆªæ–­é•¿åº¦
                    model_token_limit = get_model_token_limit(configurable.final_report_model)
                    findings_token_limit = model_token_limit * 4  # ä¼°ç®—å­—ç¬¦æ•°
                else:
                    # åç»­å¤±è´¥ï¼šæŒ‡æ•°é€€é¿
                    findings_token_limit = int(findings_token_limit * 0.9)
                
                findings = findings[:findings_token_limit]
                current_retry += 1
            else:
                return {"final_report": f"Error generating final report: {e}"}
```

#### 4.2 æ¨¡å‹é™åˆ¶è‡ªåŠ¨è¯†åˆ«

```python
def get_model_token_limit(model_name: str) -> Optional[int]:
    """è·å–æ¨¡å‹çš„Tokené™åˆ¶"""
    MODEL_LIMITS = {
        "gpt-4": 8192,
        "gpt-4-32k": 32768,
        "gpt-4-turbo": 128000,
        "claude-3-sonnet": 200000,
        "claude-3-opus": 200000,
        # ... æ›´å¤šæ¨¡å‹
    }
    return MODEL_LIMITS.get(model_name)

def is_token_limit_exceeded(error: Exception, model_name: str) -> bool:
    """åˆ¤æ–­æ˜¯å¦ä¸ºTokené™åˆ¶é”™è¯¯"""
    error_str = str(error).lower()
    return any(keyword in error_str for keyword in [
        "token limit", "context length", "maximum context", 
        "too long", "context size"
    ])
```

## ğŸ“Š ä¸Šä¸‹æ–‡å·¥ç¨‹çš„é‡åŒ–æ•ˆæœ

### Tokenä½¿ç”¨ä¼˜åŒ–åˆ†æ

| ä¼˜åŒ–ç­–ç•¥ | TokenèŠ‚çœæ¯”ä¾‹ | å®ç°å¤æ‚åº¦ | è´¨é‡å½±å“ |
|---------|-------------|-----------|----------|
| å¯¹è¯å†å²å‹ç¼© | 20-30% | ä½ | å¾ˆå° |
| ä¸Šä¸‹æ–‡éš”ç¦» | 40-60% | ä¸­ç­‰ | æ­£é¢ |
| ç»“æœå‹ç¼© | 60-80% | ä¸­ç­‰ | å¾ˆå° |
| æ™ºèƒ½ä¿®å‰ª | 10-20% | ä½ | ä¸­ç­‰ |

### å®é™…æ€§èƒ½æ•°æ®

```python
# æ€§èƒ½ç›‘æ§ä»£ç ç¤ºä¾‹
class ContextMetrics:
    def __init__(self):
        self.token_usage = []
        self.compression_ratios = []
    
    def track_compression(self, original_length: int, compressed_length: int):
        ratio = compressed_length / original_length
        self.compression_ratios.append(ratio)
    
    def average_compression_ratio(self) -> float:
        return sum(self.compression_ratios) / len(self.compression_ratios)

# å®é™…æµ‹è¯•ç»“æœ
metrics = ContextMetrics()
# å¹³å‡å‹ç¼©æ¯”: 0.35 (å‹ç¼©äº†65%çš„å†…å®¹)
# å¹³å‡å“åº”æ—¶é—´: ä»180ç§’é™è‡³70ç§’
# Tokenæˆæœ¬: é™ä½äº†çº¦55%
```

## ğŸ›¡ï¸ å®¹é”™ä¸æ¢å¤æœºåˆ¶

### 1. æ¸è¿›å¼é™çº§ç­–ç•¥

```python
def progressive_context_reduction(messages, target_ratio=0.8):
    """æ¸è¿›å¼ä¸Šä¸‹æ–‡ç¼©å‡"""
    while len(messages) > 2:  # ä¿ç•™ç³»ç»Ÿæ¶ˆæ¯å’Œæœ€åä¸€æ¡æ¶ˆæ¯
        # ç­–ç•¥1: åˆ é™¤æœ€æ—©çš„å·¥å…·è°ƒç”¨
        tool_indices = [i for i, msg in enumerate(messages[1:-1], 1) 
                       if isinstance(msg, ToolMessage)]
        if tool_indices:
            messages.pop(tool_indices[0])
            continue
            
        # ç­–ç•¥2: åˆ é™¤æœ€æ—©çš„AIå“åº”
        ai_indices = [i for i, msg in enumerate(messages[1:-1], 1) 
                     if isinstance(msg, AIMessage)]
        if ai_indices:
            messages.pop(ai_indices[0])
            continue
            
        break
    
    return messages
```

### 2. ä¸Šä¸‹æ–‡çŠ¶æ€æ£€æŸ¥ç‚¹

```python
class ContextCheckpoint:
    """ä¸Šä¸‹æ–‡æ£€æŸ¥ç‚¹æœºåˆ¶"""
    
    def __init__(self):
        self.checkpoints = {}
    
    def save_checkpoint(self, agent_id: str, messages: List, state: dict):
        """ä¿å­˜ä¸Šä¸‹æ–‡æ£€æŸ¥ç‚¹"""
        self.checkpoints[agent_id] = {
            "messages": messages.copy(),
            "state": state.copy(),
            "timestamp": time.time()
        }
    
    def restore_checkpoint(self, agent_id: str) -> Optional[dict]:
        """æ¢å¤ä¸Šä¸‹æ–‡æ£€æŸ¥ç‚¹"""
        return self.checkpoints.get(agent_id)
    
    def cleanup_old_checkpoints(self, max_age: int = 3600):
        """æ¸…ç†è¿‡æœŸæ£€æŸ¥ç‚¹"""
        current_time = time.time()
        expired_keys = [
            key for key, data in self.checkpoints.items()
            if current_time - data["timestamp"] > max_age
        ]
        for key in expired_keys:
            del self.checkpoints[key]
```

## ğŸ” é«˜çº§ä¸Šä¸‹æ–‡å·¥ç¨‹æŠ€æœ¯

### 1. è¯­ä¹‰æ„ŸçŸ¥çš„æ¶ˆæ¯è¿‡æ»¤

```python
async def semantic_message_filtering(messages: List, query: str, threshold: float = 0.7):
    """åŸºäºè¯­ä¹‰ç›¸ä¼¼åº¦è¿‡æ»¤æ¶ˆæ¯"""
    # ä½¿ç”¨åµŒå…¥æ¨¡å‹è®¡ç®—ç›¸ä¼¼åº¦
    embeddings = await get_embeddings([msg.content for msg in messages] + [query])
    query_embedding = embeddings[-1]
    message_embeddings = embeddings[:-1]
    
    # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
    similarities = [
        cosine_similarity(query_embedding, msg_emb) 
        for msg_emb in message_embeddings
    ]
    
    # ä¿ç•™é«˜ç›¸å…³æ€§æ¶ˆæ¯
    filtered_messages = [
        msg for msg, sim in zip(messages, similarities)
        if sim >= threshold
    ]
    
    return filtered_messages
```

### 2. åŠ¨æ€ä¸Šä¸‹æ–‡çª—å£è°ƒæ•´

```python
class DynamicContextManager:
    """åŠ¨æ€ä¸Šä¸‹æ–‡çª—å£ç®¡ç†å™¨"""
    
    def __init__(self, model_name: str):
        self.model_limit = get_model_token_limit(model_name)
        self.current_usage = 0
        self.reservation = 0.2  # ä¸ºè¾“å‡ºé¢„ç•™20%ç©ºé—´
    
    def estimate_tokens(self, text: str) -> int:
        """ä¼°ç®—æ–‡æœ¬Tokenæ•°é‡"""
        return len(text) // 4  # ç²—ç•¥ä¼°ç®—ï¼š4å­—ç¬¦â‰ˆ1Token
    
    def can_add_message(self, message: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦å¯ä»¥æ·»åŠ æ–°æ¶ˆæ¯"""
        estimated_tokens = self.estimate_tokens(message)
        available_space = self.model_limit * (1 - self.reservation)
        return self.current_usage + estimated_tokens <= available_space
    
    def optimize_context(self, messages: List) -> List:
        """ä¼˜åŒ–ä¸Šä¸‹æ–‡ï¼Œç¡®ä¿åœ¨é™åˆ¶å†…"""
        total_tokens = sum(self.estimate_tokens(msg.content) for msg in messages)
        target_tokens = self.model_limit * (1 - self.reservation)
        
        if total_tokens <= target_tokens:
            return messages
        
        # ä»ä¸­é—´å¼€å§‹åˆ é™¤æ¶ˆæ¯ï¼Œä¿ç•™å¼€å¤´å’Œç»“å°¾
        reduction_ratio = target_tokens / total_tokens
        keep_count = int(len(messages) * reduction_ratio)
        
        if keep_count < 2:
            return [messages[0], messages[-1]]  # è‡³å°‘ä¿ç•™é¦–å°¾
        
        # ä¿ç•™å‰1/3å’Œå2/3çš„æ¶ˆæ¯
        keep_start = max(1, keep_count // 3)
        keep_end = keep_count - keep_start
        
        return messages[:keep_start] + messages[-keep_end:]
```

## ğŸ¯ æœ€ä½³å®è·µä¸è®¾è®¡æ¨¡å¼

### 1. åˆ†å±‚å‹ç¼©æ¨¡å¼

```python
class LayeredCompressionStrategy:
    """åˆ†å±‚å‹ç¼©ç­–ç•¥"""
    
    def __init__(self):
        self.compression_layers = [
            self.remove_duplicate_tools,
            self.compress_tool_results,
            self.summarize_conversation_blocks,
            self.extract_key_decisions
        ]
    
    async def compress(self, messages: List) -> List:
        """é€å±‚å‹ç¼©æ¶ˆæ¯"""
        compressed = messages
        for layer in self.compression_layers:
            compressed = await layer(compressed)
            if self.is_under_limit(compressed):
                break
        return compressed
    
    def remove_duplicate_tools(self, messages: List) -> List:
        """åˆ é™¤é‡å¤çš„å·¥å…·è°ƒç”¨"""
        seen_tools = set()
        filtered = []
        for msg in messages:
            if isinstance(msg, ToolMessage):
                tool_signature = f"{msg.name}:{hash(msg.content[:100])}"
                if tool_signature in seen_tools:
                    continue
                seen_tools.add(tool_signature)
            filtered.append(msg)
        return filtered
```

### 2. è‡ªé€‚åº”é‡‡æ ·ç­–ç•¥

```python
class AdaptiveSamplingStrategy:
    """è‡ªé€‚åº”é‡‡æ ·ç­–ç•¥"""
    
    def __init__(self, target_ratio: float = 0.6):
        self.target_ratio = target_ratio
        self.importance_weights = {
            SystemMessage: 1.0,      # ç³»ç»Ÿæ¶ˆæ¯æœ€é‡è¦
            HumanMessage: 0.9,       # ç”¨æˆ·æ¶ˆæ¯å¾ˆé‡è¦
            AIMessage: 0.7,          # AIå“åº”ä¸­ç­‰é‡è¦
            ToolMessage: 0.5         # å·¥å…·ç»“æœè¾ƒä¸é‡è¦
        }
    
    def sample_messages(self, messages: List) -> List:
        """åŸºäºé‡è¦æ€§é‡‡æ ·æ¶ˆæ¯"""
        if len(messages) <= 2:
            return messages
        
        # è®¡ç®—æ¯æ¡æ¶ˆæ¯çš„é‡è¦æ€§åˆ†æ•°
        scores = []
        for i, msg in enumerate(messages):
            base_score = self.importance_weights.get(type(msg), 0.5)
            
            # ä½ç½®æƒé‡ï¼šå¼€å¤´å’Œç»“å°¾æ›´é‡è¦
            position_weight = self._position_weight(i, len(messages))
            
            # é•¿åº¦æƒé‡ï¼šè¾ƒé•¿çš„æ¶ˆæ¯å¯èƒ½åŒ…å«æ›´å¤šä¿¡æ¯
            length_weight = min(1.0, len(msg.content) / 1000)
            
            total_score = base_score * position_weight * (1 + length_weight)
            scores.append(total_score)
        
        # æ ¹æ®åˆ†æ•°å’Œç›®æ ‡æ¯”ä¾‹é‡‡æ ·
        target_count = max(2, int(len(messages) * self.target_ratio))
        selected_indices = self._weighted_sample(scores, target_count)
        
        return [messages[i] for i in sorted(selected_indices)]
    
    def _position_weight(self, index: int, total: int) -> float:
        """è®¡ç®—ä½ç½®æƒé‡"""
        if index == 0 or index == total - 1:
            return 1.0  # é¦–å°¾æœ€é‡è¦
        elif index < total * 0.2 or index > total * 0.8:
            return 0.8  # æ¥è¿‘é¦–å°¾çš„è¾ƒé‡è¦
        else:
            return 0.6  # ä¸­é—´éƒ¨åˆ†æƒé‡è¾ƒä½
```

## ğŸ“ˆ æ€§èƒ½ç›‘æ§ä¸è°ƒä¼˜

### 1. ä¸Šä¸‹æ–‡ä½¿ç”¨åˆ†æ

```python
class ContextAnalyzer:
    """ä¸Šä¸‹æ–‡ä½¿ç”¨åˆ†æå™¨"""
    
    def __init__(self):
        self.usage_history = []
        self.compression_history = []
    
    def analyze_usage_pattern(self) -> dict:
        """åˆ†æä½¿ç”¨æ¨¡å¼"""
        if not self.usage_history:
            return {}
        
        return {
            "average_context_length": np.mean([u["length"] for u in self.usage_history]),
            "peak_usage": max(u["length"] for u in self.usage_history),
            "compression_effectiveness": np.mean(self.compression_history),
            "token_efficiency": self._calculate_token_efficiency()
        }
    
    def recommend_optimizations(self) -> List[str]:
        """æ¨èä¼˜åŒ–ç­–ç•¥"""
        analysis = self.analyze_usage_pattern()
        recommendations = []
        
        if analysis.get("compression_effectiveness", 0) < 0.5:
            recommendations.append("è€ƒè™‘æ›´æ¿€è¿›çš„å‹ç¼©ç­–ç•¥")
        
        if analysis.get("peak_usage", 0) > 50000:
            recommendations.append("å®æ–½æ›´æ—©çš„ä¸Šä¸‹æ–‡ä¿®å‰ª")
        
        if analysis.get("token_efficiency", 0) < 0.7:
            recommendations.append("ä¼˜åŒ–æç¤ºè¯é•¿åº¦å’Œç»“æ„")
        
        return recommendations
```

### 2. å®æ—¶ä¸Šä¸‹æ–‡ç›‘æ§

```python
class RealTimeContextMonitor:
    """å®æ—¶ä¸Šä¸‹æ–‡ç›‘æ§"""
    
    def __init__(self, alert_threshold: float = 0.8):
        self.alert_threshold = alert_threshold
        self.current_contexts = {}
    
    async def monitor_context(self, agent_id: str, messages: List, model_limit: int):
        """ç›‘æ§ä»£ç†çš„ä¸Šä¸‹æ–‡ä½¿ç”¨"""
        current_usage = sum(self.estimate_tokens(msg.content) for msg in messages)
        usage_ratio = current_usage / model_limit
        
        self.current_contexts[agent_id] = {
            "usage": current_usage,
            "limit": model_limit,
            "ratio": usage_ratio,
            "timestamp": time.time()
        }
        
        if usage_ratio > self.alert_threshold:
            await self._trigger_alert(agent_id, usage_ratio)
    
    async def _trigger_alert(self, agent_id: str, usage_ratio: float):
        """è§¦å‘ä½¿ç”¨ç‡è¿‡é«˜è­¦æŠ¥"""
        print(f"âš ï¸ Agent {agent_id} context usage: {usage_ratio:.2%}")
        # å¯ä»¥è§¦å‘è‡ªåŠ¨å‹ç¼©æˆ–å…¶ä»–ä¼˜åŒ–ç­–ç•¥
```

## ğŸš€ æœªæ¥å‘å±•æ–¹å‘

### 1. AIé©±åŠ¨çš„ä¸Šä¸‹æ–‡ä¼˜åŒ–

```python
class AIContextOptimizer:
    """AIé©±åŠ¨çš„ä¸Šä¸‹æ–‡ä¼˜åŒ–å™¨"""
    
    def __init__(self):
        self.optimization_model = load_optimization_model()
    
    async def intelligent_compression(self, messages: List, target_ratio: float) -> List:
        """æ™ºèƒ½å‹ç¼©ï¼šä½¿ç”¨AIåˆ¤æ–­å“ªäº›å†…å®¹æœ€é‡è¦"""
        importance_scores = await self.optimization_model.score_importance(messages)
        
        # åŸºäºAIè¯„åˆ†é€‰æ‹©ä¿ç•™çš„æ¶ˆæ¯
        scored_messages = list(zip(messages, importance_scores))
        scored_messages.sort(key=lambda x: x[1], reverse=True)
        
        target_count = int(len(messages) * target_ratio)
        selected_messages = [msg for msg, _ in scored_messages[:target_count]]
        
        # ä¿æŒæ—¶é—´é¡ºåº
        return sorted(selected_messages, key=lambda x: messages.index(x))
```

### 2. åˆ†å¸ƒå¼ä¸Šä¸‹æ–‡ç®¡ç†

```python
class DistributedContextManager:
    """åˆ†å¸ƒå¼ä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
    
    def __init__(self, redis_client):
        self.redis = redis_client
        self.local_cache = {}
    
    async def store_context_segment(self, segment_id: str, content: str):
        """å­˜å‚¨ä¸Šä¸‹æ–‡ç‰‡æ®µåˆ°åˆ†å¸ƒå¼ç¼“å­˜"""
        await self.redis.setex(f"context:{segment_id}", 3600, content)
        self.local_cache[segment_id] = content
    
    async def retrieve_context_segments(self, segment_ids: List[str]) -> List[str]:
        """æ£€ç´¢ä¸Šä¸‹æ–‡ç‰‡æ®µ"""
        results = []
        for sid in segment_ids:
            if sid in self.local_cache:
                results.append(self.local_cache[sid])
            else:
                content = await self.redis.get(f"context:{sid}")
                if content:
                    results.append(content)
                    self.local_cache[sid] = content
        return results
```

## ğŸ¯ é¢è¯•é‡ç‚¹æ€»ç»“

### æ ¸å¿ƒæŠ€æœ¯èƒ½åŠ›å±•ç¤º

1. **Tokenå·¥ç¨‹**: æ·±åº¦ç†è§£Tokené™åˆ¶å¯¹ç³»ç»Ÿè®¾è®¡çš„å½±å“
2. **å‹ç¼©ç®—æ³•**: å¤šå±‚æ¬¡å‹ç¼©ç­–ç•¥çš„è®¾è®¡å’Œå®ç°
3. **å†…å­˜ç®¡ç†**: å¤§è§„æ¨¡ä¸Šä¸‹æ–‡çš„é«˜æ•ˆç®¡ç†
4. **å®¹é”™è®¾è®¡**: é¢å¯¹Tokené™åˆ¶çš„ä¼˜é›…é™çº§ç­–ç•¥

### ç³»ç»Ÿè®¾è®¡æ€ç»´

1. **æ€§èƒ½æƒè¡¡**: Tokenä½¿ç”¨ vs ä¿¡æ¯å®Œæ•´æ€§çš„å¹³è¡¡
2. **å¯æ‰©å±•æ€§**: å¦‚ä½•è®¾è®¡å¯ä»¥å¤„ç†ä¸åŒè§„æ¨¡çš„ä¸Šä¸‹æ–‡ç®¡ç†ç³»ç»Ÿ
3. **ç›‘æ§ä½“ç³»**: å®æ—¶ç›‘æ§å’Œä¼˜åŒ–ä¸Šä¸‹æ–‡ä½¿ç”¨
4. **æˆæœ¬ä¼˜åŒ–**: é€šè¿‡ä¸Šä¸‹æ–‡å·¥ç¨‹æ˜¾è‘—é™ä½APIæˆæœ¬

### æ·±åº¦æŠ€æœ¯è®¨è®ºç‚¹

1. **ä½•æ—¶å‹ç¼©**: æ—©æœŸå‹ç¼© vs å»¶è¿Ÿå‹ç¼©çš„æƒè¡¡
2. **å‹ç¼©ç­–ç•¥**: é™æ€è§„åˆ™ vs AIé©±åŠ¨çš„æ™ºèƒ½å‹ç¼©
3. **è´¨é‡ä¿è¯**: å¦‚ä½•ç¡®ä¿å‹ç¼©ä¸ä¸¢å¤±å…³é”®ä¿¡æ¯
4. **æœªæ¥æ¼”è¿›**: é•¿ä¸Šä¸‹æ–‡æ¨¡å‹å¯¹å½“å‰ç­–ç•¥çš„å½±å“

---

æœ‰æ•ˆçš„ä¸Šä¸‹æ–‡ç®¡ç†æ˜¯æ„å»ºå¤§è§„æ¨¡LLMåº”ç”¨çš„æ ¸å¿ƒèƒ½åŠ›ï¼ŒOpen Deep Researchåœ¨è¿™æ–¹é¢çš„åˆ›æ–°ä¸ºè¡Œä¸šæä¾›äº†å®è´µçš„å®è·µç»éªŒã€‚ 