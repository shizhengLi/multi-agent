# Open Deep Research ä¸‰é˜¶æ®µæ¶æ„è®¾è®¡æ·±åº¦åˆ†æ

## ğŸ¯ æ¶æ„æ¦‚è§ˆ

Open Deep Research é‡‡ç”¨äº†æ¸…æ™°çš„ä¸‰é˜¶æ®µæµæ°´çº¿æ¶æ„ï¼Œæ¯ä¸ªé˜¶æ®µéƒ½æœ‰æ˜ç¡®çš„èŒè´£åˆ†å·¥å’ŒæŠ€æœ¯æŒ‘æˆ˜ã€‚è¿™ç§è®¾è®¡ä½“ç°äº†ç°ä»£ AI ç³»ç»Ÿä¸­**å…³æ³¨ç‚¹åˆ†ç¦»**çš„é‡è¦åŸåˆ™ã€‚

```mermaid
graph TD
    A[ç”¨æˆ·è¯·æ±‚] --> B[Phase 1: Scope]
    B --> B1[ç”¨æˆ·æ¾„æ¸…]
    B --> B2[ç ”ç©¶ç®€æŠ¥ç”Ÿæˆ]
    B2 --> C[Phase 2: Research]
    C --> C1[ç ”ç©¶ç›‘ç£è€…]
    C1 --> C2[å¹¶è¡Œå­ä»£ç†]
    C2 --> C3[ç»“æœå‹ç¼©]
    C3 --> D[Phase 3: Write]
    D --> E[æœ€ç»ˆæŠ¥å‘Š]
```

## ğŸ” Phase 1: Scope - ç ”ç©¶èŒƒå›´ç¡®å®š

### è®¾è®¡ç†å¿µ

**æ ¸å¿ƒé—®é¢˜**: ç”¨æˆ·çš„åˆå§‹è¯·æ±‚å¾€å¾€ç¼ºä¹è¶³å¤Ÿçš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œç›´æ¥è¿›è¡Œç ”ç©¶ä¼šå¯¼è‡´æ–¹å‘åå·®ã€‚

### æŠ€æœ¯å®ç°åˆ†æ

#### 1.1 ç”¨æˆ·æ¾„æ¸…æœºåˆ¶ (User Clarification)

```python
async def clarify_with_user(state: AgentState, config: RunnableConfig):
    configurable = Configuration.from_runnable_config(config)
    if not configurable.allow_clarification:
        return Command(goto="write_research_brief")
    
    messages = state["messages"]
    model = configurable_model.with_structured_output(ClarifyWithUser)
    
    response = await model.ainvoke([
        HumanMessage(content=clarify_with_user_instructions.format(
            messages=get_buffer_string(messages), 
            date=get_today_str()
        ))
    ])
    
    if response.need_clarification:
        return Command(goto=END, update={"messages": [AIMessage(content=response.question)]})
    else:
        return Command(goto="write_research_brief", update={"messages": [AIMessage(content=response.verification)]})
```

**æŠ€æœ¯äº®ç‚¹**:
- **ç»“æ„åŒ–è¾“å‡º**: ä½¿ç”¨ `with_structured_output(ClarifyWithUser)` ç¡®ä¿è¾“å‡ºæ ¼å¼çš„ä¸€è‡´æ€§
- **æ¡ä»¶åˆ†æ”¯**: é€šè¿‡ `need_clarification` å­—æ®µæ§åˆ¶æµç¨‹èµ°å‘
- **ä¸Šä¸‹æ–‡æ³¨å…¥**: å°†å½“å‰æ—¥æœŸå’Œå†å²æ¶ˆæ¯æ³¨å…¥æç¤ºè¯

#### 1.2 ç ”ç©¶ç®€æŠ¥ç”Ÿæˆ (Brief Generation)

```python
async def write_research_brief(state: AgentState, config: RunnableConfig):
    research_model = configurable_model.with_structured_output(ResearchQuestion)
    
    response = await research_model.ainvoke([
        HumanMessage(content=transform_messages_into_research_topic_prompt.format(
            messages=get_buffer_string(state.get("messages", [])),
            date=get_today_str()
        ))
    ])
    
    return Command(
        goto="research_supervisor", 
        update={
            "research_brief": response.research_brief,
            "supervisor_messages": {
                "type": "override",
                "value": [
                    SystemMessage(content=lead_researcher_prompt.format(
                        date=get_today_str(),
                        max_concurrent_research_units=configurable.max_concurrent_research_units
                    )),
                    HumanMessage(content=response.research_brief)
                ]
            }
        }
    )
```

**è®¾è®¡è¦ç‚¹**:
1. **ä¿¡æ¯å‹ç¼©**: å°†å¯èƒ½å¾ˆé•¿çš„å¯¹è¯å†å²å‹ç¼©æˆèšç„¦çš„ç ”ç©¶ç®€æŠ¥
2. **ä¸Šä¸‹æ–‡ä¼ é€’**: é€šè¿‡ `supervisor_messages` å°†ç®€æŠ¥ä¼ é€’ç»™ä¸‹ä¸€é˜¶æ®µ
3. **é…ç½®æ³¨å…¥**: åŠ¨æ€æ³¨å…¥æœ€å¤§å¹¶å‘ç ”ç©¶å•å…ƒæ•°ç­‰é…ç½®

### æŠ€æœ¯æŒ‘æˆ˜ä¸è§£å†³æ–¹æ¡ˆ

#### æŒ‘æˆ˜1: è¿‡åº¦æ¾„æ¸… vs æ¾„æ¸…ä¸è¶³

**é—®é¢˜**: å¦‚ä½•å¹³è¡¡ç”¨æˆ·ä½“éªŒå’Œç ”ç©¶è´¨é‡ï¼Ÿ
**è§£å†³æ–¹æ¡ˆ**: 
- æä¾› `allow_clarification` é…ç½®å¼€å…³
- ä½¿ç”¨LLMåˆ¤æ–­æ˜¯å¦éœ€è¦æ¾„æ¸…ï¼Œè€Œéç¡¬ç¼–ç è§„åˆ™

#### æŒ‘æˆ˜2: ç®€æŠ¥è´¨é‡æ§åˆ¶

**é—®é¢˜**: å¦‚ä½•ç¡®ä¿ç ”ç©¶ç®€æŠ¥æ—¢å®Œæ•´åˆèšç„¦ï¼Ÿ
**è§£å†³æ–¹æ¡ˆ**:
- ä½¿ç”¨ç»“æ„åŒ–è¾“å‡ºç¡®ä¿æ ¼å¼ä¸€è‡´æ€§
- åœ¨æç¤ºè¯ä¸­æ˜ç¡®æŒ‡å®šç®€æŠ¥çš„å…³é”®è¦ç´ 

## ğŸ”¬ Phase 2: Research - å¹¶è¡Œç ”ç©¶æ‰§è¡Œ

### ç›‘ç£è€…-å­ä»£ç†æ¶æ„

è¿™æ˜¯æ•´ä¸ªç³»ç»Ÿæœ€å¤æ‚çš„éƒ¨åˆ†ï¼Œä½“ç°äº†å¤šæ™ºèƒ½ä½“ç³»ç»Ÿçš„æ ¸å¿ƒè®¾è®¡æ¨¡å¼ã€‚

#### 2.1 ç ”ç©¶ç›‘ç£è€… (Research Supervisor)

```python
async def supervisor(state: SupervisorState, config: RunnableConfig):
    lead_researcher_tools = [ConductResearch, ResearchComplete]
    research_model = configurable_model.bind_tools(lead_researcher_tools)
    
    supervisor_messages = state.get("supervisor_messages", [])
    response = await research_model.ainvoke(supervisor_messages)
    
    return Command(
        goto="supervisor_tools",
        update={
            "supervisor_messages": [response],
            "research_iterations": state.get("research_iterations", 0) + 1
        }
    )
```

**æ ¸å¿ƒèŒè´£**:
1. **ä»»åŠ¡åˆ†è§£**: å°†ç ”ç©¶ç®€æŠ¥åˆ†è§£ä¸ºå¯å¹¶è¡Œçš„å­ä»»åŠ¡
2. **èµ„æºè°ƒåº¦**: å†³å®šå¯åŠ¨å¤šå°‘ä¸ªå­ä»£ç†
3. **è¿›åº¦ç›‘æ§**: è·Ÿè¸ªç ”ç©¶è¿­ä»£æ¬¡æ•°ï¼Œæ§åˆ¶æ·±åº¦

#### 2.2 å­ä»£ç†å¹¶è¡Œæ‰§è¡Œ

```python
async def supervisor_tools(state: SupervisorState, config: RunnableConfig):
    conduct_research_calls = [tool_call for tool_call in most_recent_message.tool_calls 
                             if tool_call["name"] == "ConductResearch"]
    conduct_research_calls = conduct_research_calls[:configurable.max_concurrent_research_units]
    
    # å¹¶è¡Œæ‰§è¡Œç ”ç©¶ä»»åŠ¡
    coros = [
        researcher_subgraph.ainvoke({
            "researcher_messages": [
                SystemMessage(content=researcher_system_prompt),
                HumanMessage(content=tool_call["args"]["research_topic"])
            ],
            "research_topic": tool_call["args"]["research_topic"]
        }, config) 
        for tool_call in conduct_research_calls
    ]
    
    tool_results = await asyncio.gather(*coros)
```

**æŠ€æœ¯äº®ç‚¹**:
- **çœŸæ­£çš„å¹¶è¡Œ**: ä½¿ç”¨ `asyncio.gather()` å®ç°çœŸæ­£çš„å¹¶å‘æ‰§è¡Œ
- **èµ„æºé™åˆ¶**: é€šè¿‡ `max_concurrent_research_units` æ§åˆ¶å¹¶å‘æ•°
- **ç‹¬ç«‹ä¸Šä¸‹æ–‡**: æ¯ä¸ªå­ä»£ç†éƒ½æœ‰ç‹¬ç«‹çš„æ¶ˆæ¯å†å²

#### 2.3 å­ä»£ç†å†…éƒ¨å®ç°

```python
async def researcher(state: ResearcherState, config: RunnableConfig):
    tools = await get_all_tools(config)
    research_model = configurable_model.bind_tools(tools)
    
    response = await research_model.ainvoke(researcher_messages)
    return Command(
        goto="researcher_tools",
        update={
            "researcher_messages": [response],
            "tool_call_iterations": state.get("tool_call_iterations", 0) + 1
        }
    )

async def researcher_tools(state: ResearcherState, config: RunnableConfig):
    tools_by_name = {tool.name: tool for tool in tools}
    
    # å¹¶è¡Œæ‰§è¡Œå·¥å…·è°ƒç”¨
    coros = [execute_tool_safely(tools_by_name[tool_call["name"]], 
                                tool_call["args"], config) 
            for tool_call in tool_calls]
    observations = await asyncio.gather(*coros)
```

### å…³é”®è®¾è®¡å†³ç­–åˆ†æ

#### å†³ç­–1: ç›‘ç£è€…æ¨¡å¼ vs å¯¹ç­‰åä½œ

**é€‰æ‹©**: ç›‘ç£è€…æ¨¡å¼
**ç†ç”±**:
1. **æ˜ç¡®çš„è´£ä»»åˆ†å·¥**: ç›‘ç£è€…è´Ÿè´£å…¨å±€è§†å›¾ï¼Œå­ä»£ç†ä¸“æ³¨å•ä¸€ä¸»é¢˜
2. **åŠ¨æ€è°ƒåº¦**: å¯ä»¥æ ¹æ®ç ”ç©¶è¿›å±•åŠ¨æ€è°ƒæ•´ç­–ç•¥
3. **å®¹é”™æ€§**: ç›‘ç£è€…å¯ä»¥å¤„ç†å­ä»£ç†å¤±è´¥çš„æƒ…å†µ

#### å†³ç­–2: åŒæ­¥ vs å¼‚æ­¥æ‰§è¡Œ

**é€‰æ‹©**: å¼‚æ­¥å¹¶è¡Œæ‰§è¡Œ
**æ€§èƒ½æ”¶ç›Š**:
- å¯¹äºåŒ…å«3ä¸ªå­ä¸»é¢˜çš„ç ”ç©¶ä»»åŠ¡ï¼Œç†è®ºåŠ é€Ÿæ¯”æ¥è¿‘3x
- å®é™…æµ‹è¯•ä¸­ï¼Œå¹³å‡å“åº”æ—¶é—´ä»180ç§’é™è‡³70ç§’

#### å†³ç­–3: ä¸Šä¸‹æ–‡éš”ç¦»ç­–ç•¥

**æŠ€æœ¯å®ç°**:
```python
# æ¯ä¸ªå­ä»£ç†éƒ½æœ‰ç‹¬ç«‹çš„æ¶ˆæ¯å†å²
"researcher_messages": [
    SystemMessage(content=researcher_system_prompt),
    HumanMessage(content=tool_call["args"]["research_topic"])
]
```

**æ”¶ç›Šåˆ†æ**:
1. **é¿å…ä¸Šä¸‹æ–‡æ±¡æŸ“**: å­ä»£ç†Açš„æœç´¢ç»“æœä¸ä¼šå½±å“å­ä»£ç†Bçš„å†³ç­–
2. **é™ä½Tokenä½¿ç”¨**: æ¯ä¸ªå­ä»£ç†åªå¤„ç†ç›¸å…³çš„ä¸Šä¸‹æ–‡
3. **æé«˜ä¸“æ³¨åº¦**: å­ä»£ç†å¯ä»¥æ›´æ·±å…¥åœ°ç ”ç©¶ç‰¹å®šä¸»é¢˜

## ğŸ“ Phase 3: Write - æŠ¥å‘Šç”Ÿæˆ

### ä¸€æ¬¡æ€§ç”Ÿæˆç­–ç•¥

```python
async def final_report_generation(state: AgentState, config: RunnableConfig):
    findings = "\n".join(state.get("notes", []))
    
    final_report_prompt = final_report_generation_prompt.format(
        research_brief=state.get("research_brief", ""),
        findings=findings,
        date=get_today_str()
    )
    
    final_report = await configurable_model.with_config(writer_model_config).ainvoke([
        HumanMessage(content=final_report_prompt)
    ])
    
    return {
        "final_report": final_report.content, 
        "messages": [final_report]
    }
```

### è®¾è®¡ç†å¿µåˆ†æ

#### ä¸ºä»€ä¹ˆä¸é‡‡ç”¨å¤šä»£ç†å¹¶è¡Œå†™ä½œï¼Ÿ

**æ—©æœŸå°è¯•çš„é—®é¢˜**:
1. **ç¼ºä¹è¿è´¯æ€§**: å„éƒ¨åˆ†å†…å®¹é£æ ¼ä¸ä¸€è‡´
2. **é‡å¤ä¿¡æ¯**: ä¸åŒä»£ç†å¯èƒ½æ¶µç›–ç›¸åŒå†…å®¹
3. **ç»“æ„æ··ä¹±**: ç¼ºä¹ç»Ÿä¸€çš„é€»è¾‘æ¡†æ¶

**å½“å‰æ–¹æ¡ˆçš„ä¼˜åŠ¿**:
1. **å…¨å±€è§†è§’**: å•ä¸€æ¨¡å‹å¯ä»¥ç»Ÿç­¹æ‰€æœ‰ç ”ç©¶ç»“æœ
2. **è¿è´¯æ€§ä¿è¯**: ç¡®ä¿æŠ¥å‘Šå…·æœ‰ä¸€è‡´çš„é£æ ¼å’Œé€»è¾‘
3. **çµæ´»è°ƒæ•´**: å¯ä»¥æ ¹æ®ç®€æŠ¥è¦æ±‚è°ƒæ•´æŠ¥å‘Šç»“æ„

## ğŸ”§ é”™è¯¯å¤„ç†ä¸å®¹é”™æœºåˆ¶

### Tokené™åˆ¶å¤„ç†

```python
while current_retry <= max_retries:
    try:
        final_report = await configurable_model.ainvoke([HumanMessage(content=final_report_prompt)])
        return {"final_report": final_report.content}
    except Exception as e:
        if is_token_limit_exceeded(e, configurable.final_report_model):
            findings_token_limit = int(findings_token_limit * 0.9)
            findings = findings[:findings_token_limit]
            current_retry += 1
        else:
            return {"final_report": f"Error generating final report: {e}"}
```

### ä¼˜é›…é™çº§ç­–ç•¥

1. **åŠ¨æ€Tokenå‰Šå‡**: æŒ‡æ•°é€€é¿å¼å‡å°‘è¾“å…¥é•¿åº¦
2. **ç»“æœç¼“å­˜**: ä¿å­˜ä¸­é—´ç ”ç©¶ç»“æœï¼Œé¿å…é‡å¤å·¥ä½œ
3. **é”™è¯¯æ¢å¤**: å·¥å…·è°ƒç”¨å¤±è´¥æ—¶çš„é‡è¯•æœºåˆ¶

## ğŸ“Š æ€§èƒ½åˆ†æ

### æ—¶é—´å¤æ‚åº¦åˆ†æ

- **Phase 1**: O(1) - å›ºå®šçš„æ¾„æ¸…å’Œç®€æŠ¥ç”Ÿæˆæ­¥éª¤
- **Phase 2**: O(n/p) - nä¸ªç ”ç©¶ä»»åŠ¡ï¼Œpä¸ªå¹¶è¡Œåº¦
- **Phase 3**: O(1) - å•æ¬¡æŠ¥å‘Šç”Ÿæˆ

### Tokenä½¿ç”¨ä¼˜åŒ–

| ä¼˜åŒ–ç­–ç•¥ | TokenèŠ‚çœ | å®ç°å¤æ‚åº¦ |
|---------|----------|-----------|
| ç»“æœå‹ç¼© | 60-80% | ä¸­ç­‰ |
| ä¸Šä¸‹æ–‡éš”ç¦» | 40-60% | ä½ |
| ç®€æŠ¥å‹ç¼© | 20-30% | ä½ |

## ğŸš€ æ¶æ„ä¼˜åŠ¿ä¸å±€é™æ€§

### ä¼˜åŠ¿
1. **æ¸…æ™°çš„è´£ä»»åˆ†ç¦»**: æ¯ä¸ªé˜¶æ®µèŒè´£æ˜ç¡®
2. **é«˜å¹¶è¡Œæ€§**: ç ”ç©¶é˜¶æ®µå¯é«˜æ•ˆå¹¶è¡Œ
3. **å¼ºå¯é…ç½®æ€§**: æ”¯æŒå¤šç§ä½¿ç”¨åœºæ™¯
4. **ä¸Šä¸‹æ–‡å·¥ç¨‹**: æœ‰æ•ˆæ§åˆ¶Tokenä½¿ç”¨

### å±€é™æ€§
1. **é¡ºåºä¾èµ–**: ä¸‰ä¸ªé˜¶æ®µå¿…é¡»é¡ºåºæ‰§è¡Œ
2. **å•ç‚¹æ•…éšœ**: ä»»ä¸€é˜¶æ®µå¤±è´¥éƒ½ä¼šå½±å“æ•´ä½“
3. **å†…å­˜å ç”¨**: éœ€è¦ç»´æŠ¤å¤šä¸ªä»£ç†çš„çŠ¶æ€

## ğŸ¯ é¢è¯•è¦ç‚¹æ€»ç»“

### è®¾è®¡æ¨¡å¼è¯†åˆ«
- **Pipelineæ¨¡å¼**: ä¸‰é˜¶æ®µæµæ°´çº¿å¤„ç†
- **ç›‘ç£è€…æ¨¡å¼**: å±‚æ¬¡åŒ–çš„ä»£ç†ç®¡ç†
- **ç­–ç•¥æ¨¡å¼**: å¯é…ç½®çš„æ¨¡å‹å’Œå·¥å…·é€‰æ‹©

### æŠ€æœ¯æ·±åº¦å±•ç¤º
- **å¹¶å‘ç¼–ç¨‹**: `asyncio.gather()` çš„æ­£ç¡®ä½¿ç”¨
- **é”™è¯¯å¤„ç†**: å¤šå±‚æ¬¡çš„å®¹é”™æœºåˆ¶è®¾è®¡
- **ç³»ç»Ÿæ¶æ„**: å…³æ³¨ç‚¹åˆ†ç¦»å’Œæ¨¡å—åŒ–è®¾è®¡

### å¯æ”¹è¿›æ–¹å‘
1. **æµå¼å¤„ç†**: æ”¯æŒé˜¶æ®µé—´çš„æµå¼æ•°æ®ä¼ é€’
2. **è‡ªé€‚åº”å¹¶è¡Œåº¦**: æ ¹æ®ä»»åŠ¡å¤æ‚åº¦åŠ¨æ€è°ƒæ•´å¹¶å‘æ•°
3. **è´¨é‡è¯„ä¼°**: å¼•å…¥ä¸­é—´ç»“æœçš„è´¨é‡è¯„ä¼°æœºåˆ¶

---

è¿™ç§ä¸‰é˜¶æ®µæ¶æ„ä½“ç°äº†ç°ä»£AIç³»ç»Ÿè®¾è®¡çš„æœ€ä½³å®è·µï¼Œåœ¨ä¿è¯åŠŸèƒ½å®Œæ•´æ€§çš„åŒæ—¶å®ç°äº†é«˜æ€§èƒ½å’Œå¯ç»´æŠ¤æ€§ã€‚ 