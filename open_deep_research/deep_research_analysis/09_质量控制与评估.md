# Open Deep Research è´¨é‡æ§åˆ¶ä¸è¯„ä¼°æ·±åº¦åˆ†æ

## ğŸ¯ è´¨é‡ä¿è¯ä½“ç³»æ¦‚è§ˆ

Open Deep Researchæ„å»ºäº†ä¸€ä¸ª**å¤šç»´åº¦è´¨é‡æ§åˆ¶ä½“ç³»**ï¼Œä»è¾“å…¥éªŒè¯åˆ°è¾“å‡ºè¯„ä¼°ï¼Œç¡®ä¿ç ”ç©¶ç»“æœçš„**å‡†ç¡®æ€§ã€å®Œæ•´æ€§å’Œå¯é æ€§**ã€‚è¿™ä¸ªä½“ç³»ä½“ç°äº†ç°ä»£AIç³»ç»Ÿä¸­**è´¨é‡å·¥ç¨‹**å’Œ**å¯ä¿¡AI**çš„æ ¸å¿ƒç†å¿µã€‚

```mermaid
graph TB
    subgraph "è´¨é‡æ§åˆ¶ä½“ç³»æ¶æ„"
        A[è¾“å…¥è´¨é‡æ£€æŸ¥] -->|éªŒè¯| B[æ¾„æ¸…ä¸è§„èŒƒåŒ–]
        B -->|ç”Ÿæˆ| C[ç ”ç©¶ç®€æŠ¥è´¨é‡]
        C -->|åˆ†è§£| D[ä»»åŠ¡è´¨é‡éªŒè¯]
        
        subgraph "æ‰§è¡Œè´¨é‡ç›‘æ§"
            E[å·¥å…·è°ƒç”¨è´¨é‡]
            F[ä¿¡æ¯æ”¶é›†è´¨é‡]
            G[ä¸Šä¸‹æ–‡ç®¡ç†è´¨é‡]
        end
        
        subgraph "è¾“å‡ºè´¨é‡è¯„ä¼°"
            H[å†…å®¹å®Œæ•´æ€§]
            I[ä¿¡æ¯å‡†ç¡®æ€§]
            J[é€»è¾‘ä¸€è‡´æ€§]
            K[æ¥æºå¯é æ€§]
        end
        
        subgraph "æŒç»­æ”¹è¿›æœºåˆ¶"
            L[è´¨é‡åé¦ˆå¾ªç¯]
            M[æ¨¡å¼è¯†åˆ«]
            N[è‡ªåŠ¨åŒ–ä¼˜åŒ–]
        end
        
        D --> E
        D --> F
        D --> G
        
        E --> H
        F --> I
        G --> J
        
        H --> L
        I --> M
        J --> N
        K --> L
    end
```

## ğŸ” è¾“å…¥è´¨é‡æ§åˆ¶

### 1. éœ€æ±‚æ¾„æ¸…çš„è´¨é‡æœºåˆ¶

#### 1.1 æ™ºèƒ½æ¾„æ¸…å†³ç­–

```python
class IntelligentClarificationEngine:
    """æ™ºèƒ½æ¾„æ¸…å¼•æ“"""
    
    def __init__(self, llm_model):
        self.llm_model = llm_model
        self.clarification_patterns = {
            "ambiguous_terms": r'\b(?:it|this|that|they|those)\b',
            "vague_quantities": r'\b(?:some|many|few|several|most)\b',
            "unclear_scope": r'\b(?:about|around|regarding|concerning)\b',
            "missing_context": r'\b(?:compare|analyze|evaluate)\b\s+\w+$'
        }
    
    async def evaluate_clarification_need(self, messages: List[dict]) -> dict:
        """è¯„ä¼°æ˜¯å¦éœ€è¦æ¾„æ¸…ä»¥åŠæ¾„æ¸…è´¨é‡"""
        
        # 1. ç»“æ„åŒ–åˆ†æç”¨æˆ·è¯·æ±‚
        user_request = self._extract_user_request(messages)
        
        # 2. å¤šç»´åº¦è´¨é‡æ£€æŸ¥
        quality_checks = await asyncio.gather(
            self._check_specificity(user_request),
            self._check_completeness(user_request),
            self._check_feasibility(user_request),
            self._check_scope_clarity(user_request)
        )
        
        # 3. ç»¼åˆè´¨é‡è¯„åˆ†
        quality_scores = {
            "specificity": quality_checks[0],
            "completeness": quality_checks[1], 
            "feasibility": quality_checks[2],
            "scope_clarity": quality_checks[3]
        }
        
        overall_quality = sum(quality_scores.values()) / len(quality_scores)
        
        # 4. å†³ç­–é€»è¾‘
        if overall_quality < 0.6:
            clarification_strategy = await self._generate_clarification_strategy(quality_scores, user_request)
            return {
                "need_clarification": True,
                "quality_score": overall_quality,
                "weak_dimensions": [k for k, v in quality_scores.items() if v < 0.5],
                "clarification_strategy": clarification_strategy
            }
        else:
            return {
                "need_clarification": False,
                "quality_score": overall_quality,
                "validation_message": self._generate_validation_message(user_request)
            }
    
    async def _check_specificity(self, request: str) -> float:
        """æ£€æŸ¥è¯·æ±‚çš„å…·ä½“æ€§"""
        specificity_prompt = f"""
        è¯„ä¼°ä»¥ä¸‹ç ”ç©¶è¯·æ±‚çš„å…·ä½“æ€§ç¨‹åº¦(0-1)ï¼š
        
        è¯·æ±‚: {request}
        
        è¯„ä¼°æ ‡å‡†:
        - æ˜¯å¦æœ‰æ˜ç¡®çš„ç ”ç©¶å¯¹è±¡
        - æ˜¯å¦æœ‰å…·ä½“çš„ç ”ç©¶ç»´åº¦
        - æ˜¯å¦æœ‰æ˜ç¡®çš„äº§å‡ºæœŸæœ›
        - æ˜¯å¦é¿å…äº†æ¨¡ç³Šè¯æ±‡
        
        å…·ä½“æ€§è¯„åˆ†(0-1): """
        
        response = await self.llm_model.ainvoke([HumanMessage(content=specificity_prompt)])
        return self._parse_score(response.content)
    
    async def _check_completeness(self, request: str) -> float:
        """æ£€æŸ¥è¯·æ±‚çš„å®Œæ•´æ€§"""
        completeness_indicators = [
            "æ˜ç¡®çš„ç ”ç©¶ç›®æ ‡",
            "å…·ä½“çš„æ¯”è¾ƒå¯¹è±¡ï¼ˆå¦‚æœæ˜¯æ¯”è¾ƒç ”ç©¶ï¼‰",
            "æœŸæœ›çš„æ·±åº¦çº§åˆ«",
            "ç‰¹å®šçš„åº”ç”¨åœºæ™¯æˆ–ç”¨é€”",
            "æ—¶é—´èŒƒå›´æˆ–ç‰ˆæœ¬é™åˆ¶"
        ]
        
        completeness_prompt = f"""
        è¯„ä¼°ç ”ç©¶è¯·æ±‚çš„å®Œæ•´æ€§ï¼š
        
        è¯·æ±‚: {request}
        
        æ£€æŸ¥æ˜¯å¦åŒ…å«ä»¥ä¸‹è¦ç´ ï¼š
        {chr(10).join(f"- {indicator}" for indicator in completeness_indicators)}
        
        å®Œæ•´æ€§è¯„åˆ†(0-1ï¼Œ1è¡¨ç¤ºåŒ…å«æ‰€æœ‰å…³é”®è¦ç´ ): """
        
        response = await self.llm_model.ainvoke([HumanMessage(content=completeness_prompt)])
        return self._parse_score(response.content)
    
    async def _generate_clarification_strategy(self, quality_scores: dict, request: str) -> dict:
        """ç”Ÿæˆé’ˆå¯¹æ€§çš„æ¾„æ¸…ç­–ç•¥"""
        
        # è¯†åˆ«æœ€éœ€è¦æ¾„æ¸…çš„ç»´åº¦
        weakest_dimension = min(quality_scores.items(), key=lambda x: x[1])
        
        clarification_templates = {
            "specificity": "è¯·æä¾›æ›´å…·ä½“çš„{ç ”ç©¶å¯¹è±¡/æ¯”è¾ƒç»´åº¦/æœŸæœ›äº§å‡º}",
            "completeness": "ä¸ºäº†æ›´å¥½åœ°å®Œæˆç ”ç©¶ï¼Œè¯·è¡¥å……{ç¼ºå¤±çš„å…³é”®ä¿¡æ¯}",
            "feasibility": "è¯·ç¡®è®¤ç ”ç©¶çš„{æ—¶é—´èŒƒå›´/èµ„æºé™åˆ¶/æ•°æ®å¯è·å¾—æ€§}",
            "scope_clarity": "è¯·æ˜ç¡®ç ”ç©¶çš„{è¾¹ç•ŒèŒƒå›´/é‡ç‚¹é¢†åŸŸ/æ’é™¤å†…å®¹}"
        }
        
        base_template = clarification_templates.get(weakest_dimension[0], "è¯·æä¾›æ›´å¤šå…·ä½“ä¿¡æ¯")
        
        # ä½¿ç”¨LLMç”Ÿæˆä¸ªæ€§åŒ–æ¾„æ¸…é—®é¢˜
        clarification_prompt = f"""
        åŸºäºä»¥ä¸‹åˆ†æä¸ºç”¨æˆ·ç”Ÿæˆæ¾„æ¸…é—®é¢˜ï¼š
        
        ç”¨æˆ·è¯·æ±‚: {request}
        è´¨é‡åˆ†æ: {quality_scores}
        æœ€å¼±ç»´åº¦: {weakest_dimension[0]}
        
        ç”Ÿæˆ1-2ä¸ªå…·ä½“çš„æ¾„æ¸…é—®é¢˜ï¼Œå¸®åŠ©ç”¨æˆ·æä¾›å¿…è¦ä¿¡æ¯ï¼š
        """
        
        response = await self.llm_model.ainvoke([HumanMessage(content=clarification_prompt)])
        
        return {
            "primary_question": response.content,
            "focus_dimension": weakest_dimension[0],
            "quality_gap": 0.6 - weakest_dimension[1]
        }
```

#### 1.2 ç ”ç©¶ç®€æŠ¥è´¨é‡éªŒè¯

```python
class ResearchBriefValidator:
    """ç ”ç©¶ç®€æŠ¥è´¨é‡éªŒè¯å™¨"""
    
    def __init__(self, llm_model):
        self.llm_model = llm_model
        self.quality_criteria = {
            "clarity": 0.8,      # æ¸…æ™°åº¦é˜ˆå€¼
            "actionability": 0.7, # å¯æ‰§è¡Œæ€§é˜ˆå€¼
            "scope": 0.75,       # èŒƒå›´åˆç†æ€§é˜ˆå€¼
            "decomposability": 0.7 # å¯åˆ†è§£æ€§é˜ˆå€¼
        }
    
    async def validate_research_brief(self, brief: str, original_messages: List[dict]) -> dict:
        """éªŒè¯ç ”ç©¶ç®€æŠ¥è´¨é‡"""
        
        # 1. å¤šç»´åº¦è´¨é‡è¯„ä¼°
        quality_assessments = await asyncio.gather(
            self._assess_clarity(brief),
            self._assess_actionability(brief),
            self._assess_scope_appropriateness(brief),
            self._assess_decomposability(brief)
        )
        
        quality_scores = dict(zip(self.quality_criteria.keys(), quality_assessments))
        
        # 2. ä¸€è‡´æ€§æ£€æŸ¥
        consistency_score = await self._check_consistency_with_original(brief, original_messages)
        
        # 3. å¯è¡Œæ€§éªŒè¯
        feasibility_score = await self._assess_research_feasibility(brief)
        
        # 4. ç»¼åˆè´¨é‡åˆ¤æ–­
        overall_quality = (
            sum(quality_scores.values()) * 0.6 +
            consistency_score * 0.25 +
            feasibility_score * 0.15
        ) / 3
        
        # 5. è´¨é‡æŠ¥å‘Šç”Ÿæˆ
        quality_report = {
            "overall_quality": overall_quality,
            "dimension_scores": quality_scores,
            "consistency_score": consistency_score,
            "feasibility_score": feasibility_score,
            "passes_threshold": all(score >= threshold for score, threshold in zip(quality_scores.values(), self.quality_criteria.values())),
            "improvement_suggestions": await self._generate_improvement_suggestions(quality_scores)
        }
        
        return quality_report
    
    async def _assess_clarity(self, brief: str) -> float:
        """è¯„ä¼°ç®€æŠ¥æ¸…æ™°åº¦"""
        clarity_prompt = f"""
        è¯„ä¼°ä»¥ä¸‹ç ”ç©¶ç®€æŠ¥çš„æ¸…æ™°åº¦(0-1)ï¼š
        
        ç®€æŠ¥: {brief}
        
        è¯„ä¼°ç»´åº¦:
        - è¯­è¨€è¡¨è¾¾æ˜¯å¦æ¸…æ™°å‡†ç¡®
        - ç ”ç©¶ç›®æ ‡æ˜¯å¦æ˜ç¡®
        - æœŸæœ›äº§å‡ºæ˜¯å¦å…·ä½“
        - æ˜¯å¦é¿å…æ­§ä¹‰è¡¨è¾¾
        
        æ¸…æ™°åº¦è¯„åˆ†(0-1): """
        
        response = await self.llm_model.ainvoke([HumanMessage(content=clarity_prompt)])
        return self._parse_score(response.content)
    
    async def _assess_decomposability(self, brief: str) -> float:
        """è¯„ä¼°ç®€æŠ¥çš„å¯åˆ†è§£æ€§"""
        decomposability_prompt = f"""
        è¯„ä¼°ç ”ç©¶ç®€æŠ¥æ˜¯å¦é€‚åˆåˆ†è§£ä¸ºå¹¶è¡Œå­ä»»åŠ¡(0-1)ï¼š
        
        ç®€æŠ¥: {brief}
        
        è¯„ä¼°æ ‡å‡†:
        - æ˜¯å¦å¯ä»¥è¯†åˆ«ç‹¬ç«‹çš„ç ”ç©¶ç»´åº¦
        - å­ä»»åŠ¡ä¹‹é—´çš„ä¾èµ–å…³ç³»æ˜¯å¦æœ€å°
        - æ˜¯å¦é€‚åˆå¹¶è¡Œæ‰§è¡Œ
        - åˆ†è§£åçš„ä»»åŠ¡æ˜¯å¦å…·æœ‰åˆé€‚çš„ç²’åº¦
        
        å¯åˆ†è§£æ€§è¯„åˆ†(0-1): """
        
        response = await self.llm_model.ainvoke([HumanMessage(content=decomposability_prompt)])
        return self._parse_score(response.content)
```

## ğŸ”¬ æ‰§è¡Œè¿‡ç¨‹è´¨é‡ç›‘æ§

### 1. å®æ—¶è´¨é‡ç›‘æ§ç³»ç»Ÿ

#### 1.1 å·¥å…·è°ƒç”¨è´¨é‡ç›‘æ§

```python
class ToolExecutionQualityMonitor:
    """å·¥å…·æ‰§è¡Œè´¨é‡ç›‘æ§å™¨"""
    
    def __init__(self):
        self.quality_metrics = {
            "success_rate": 0.95,      # æˆåŠŸç‡é˜ˆå€¼
            "response_relevance": 0.8,  # å“åº”ç›¸å…³æ€§é˜ˆå€¼  
            "information_density": 0.6, # ä¿¡æ¯å¯†åº¦é˜ˆå€¼
            "source_reliability": 0.7   # æ¥æºå¯é æ€§é˜ˆå€¼
        }
        self.quality_history = []
    
    async def monitor_tool_execution(self, tool_name: str, inputs: dict, outputs: str, context: dict) -> dict:
        """ç›‘æ§å•æ¬¡å·¥å…·æ‰§è¡Œè´¨é‡"""
        
        # 1. åŸºç¡€è´¨é‡æŒ‡æ ‡
        execution_success = not outputs.startswith("Error")
        response_length = len(outputs)
        
        # 2. æ·±åº¦è´¨é‡åˆ†æ
        quality_scores = await asyncio.gather(
            self._assess_response_relevance(inputs, outputs, context),
            self._assess_information_density(outputs),
            self._assess_source_reliability(outputs, tool_name),
            self._assess_content_freshness(outputs)
        )
        
        quality_assessment = {
            "tool_name": tool_name,
            "execution_success": execution_success,
            "response_length": response_length,
            "relevance_score": quality_scores[0],
            "density_score": quality_scores[1],
            "reliability_score": quality_scores[2],
            "freshness_score": quality_scores[3],
            "timestamp": time.time()
        }
        
        # 3. ç»¼åˆè´¨é‡è¯„åˆ†
        overall_quality = self._calculate_overall_quality(quality_assessment)
        quality_assessment["overall_quality"] = overall_quality
        
        # 4. è´¨é‡é¢„è­¦
        quality_alerts = self._check_quality_alerts(quality_assessment)
        
        # 5. è®°å½•å†å²
        self.quality_history.append(quality_assessment)
        
        return {
            "quality_assessment": quality_assessment,
            "quality_alerts": quality_alerts,
            "improvement_recommendations": await self._generate_tool_improvement_recommendations(quality_assessment)
        }
    
    async def _assess_response_relevance(self, inputs: dict, outputs: str, context: dict) -> float:
        """è¯„ä¼°å“åº”ç›¸å…³æ€§"""
        relevance_prompt = f"""
        è¯„ä¼°å·¥å…·å“åº”ä¸è¾“å…¥æŸ¥è¯¢çš„ç›¸å…³æ€§(0-1)ï¼š
        
        è¾“å…¥æŸ¥è¯¢: {inputs}
        å·¥å…·å“åº”: {outputs[:1000]}...
        ç ”ç©¶ä¸Šä¸‹æ–‡: {context.get('research_topic', '')}
        
        è¯„ä¼°æ ‡å‡†:
        - å“åº”æ˜¯å¦ç›´æ¥å›ç­”äº†æŸ¥è¯¢é—®é¢˜
        - å†…å®¹æ˜¯å¦ä¸ç ”ç©¶ä¸»é¢˜ç›¸å…³
        - ä¿¡æ¯æ˜¯å¦å…·æœ‰å®ç”¨ä»·å€¼
        - æ˜¯å¦åŒ…å«æ— å…³æˆ–å™ªéŸ³ä¿¡æ¯
        
        ç›¸å…³æ€§è¯„åˆ†(0-1): """
        
        # å®é™…å®ç°ä¸­åº”è¯¥è°ƒç”¨LLM
        # response = await self.llm_model.ainvoke([HumanMessage(content=relevance_prompt)])
        # return self._parse_score(response.content)
        
        # ç®€åŒ–å®ç°ï¼šåŸºäºå…³é”®è¯åŒ¹é…çš„ç›¸å…³æ€§åˆ†æ
        query_keywords = set(str(inputs).lower().split())
        response_keywords = set(outputs.lower().split())
        
        if not query_keywords:
            return 0.5
        
        overlap = len(query_keywords & response_keywords)
        return min(1.0, overlap / len(query_keywords))
    
    async def _assess_information_density(self, outputs: str) -> float:
        """è¯„ä¼°ä¿¡æ¯å¯†åº¦"""
        if not outputs or len(outputs) < 50:
            return 0.1
        
        # ç®€å•çš„ä¿¡æ¯å¯†åº¦è¯„ä¼°
        words = outputs.split()
        unique_words = set(word.lower() for word in words if len(word) > 3)
        
        # ä¿¡æ¯å¯†åº¦ = ç‹¬ç‰¹è¯æ±‡æ¯”ä¾‹ * é•¿åº¦å› å­
        uniqueness_ratio = len(unique_words) / max(len(words), 1)
        length_factor = min(1.0, len(outputs) / 1000)  # 1000å­—ç¬¦ä¸ºåŸºå‡†
        
        return min(1.0, uniqueness_ratio * 2 * length_factor)
    
    def _check_quality_alerts(self, assessment: dict) -> List[dict]:
        """æ£€æŸ¥è´¨é‡é¢„è­¦"""
        alerts = []
        
        # æˆåŠŸç‡é¢„è­¦
        if not assessment["execution_success"]:
            alerts.append({
                "type": "execution_failure",
                "severity": "high",
                "message": f"å·¥å…· {assessment['tool_name']} æ‰§è¡Œå¤±è´¥"
            })
        
        # ç›¸å…³æ€§é¢„è­¦
        if assessment["relevance_score"] < self.quality_metrics["response_relevance"]:
            alerts.append({
                "type": "low_relevance",
                "severity": "medium",
                "message": f"å“åº”ç›¸å…³æ€§è¿‡ä½: {assessment['relevance_score']:.2f}"
            })
        
        # ä¿¡æ¯å¯†åº¦é¢„è­¦
        if assessment["density_score"] < self.quality_metrics["information_density"]:
            alerts.append({
                "type": "low_information_density",
                "severity": "medium", 
                "message": f"ä¿¡æ¯å¯†åº¦ä¸è¶³: {assessment['density_score']:.2f}"
            })
        
        return alerts
```

#### 1.2 ç ”ç©¶è¿›å±•è´¨é‡è¿½è¸ª

```python
class ResearchProgressQualityTracker:
    """ç ”ç©¶è¿›å±•è´¨é‡è¿½è¸ªå™¨"""
    
    def __init__(self):
        self.progress_checkpoints = []
        self.quality_trends = {}
        self.completeness_indicators = {
            "breadth_coverage": 0.8,    # å¹¿åº¦è¦†ç›–åº¦
            "depth_sufficiency": 0.75,  # æ·±åº¦å……åˆ†æ€§
            "source_diversity": 0.7,    # æ¥æºå¤šæ ·æ€§
            "evidence_strength": 0.8    # è¯æ®å¼ºåº¦
        }
    
    async def track_research_progress(self, current_results: List[str], research_brief: str, iteration: int) -> dict:
        """è¿½è¸ªç ”ç©¶è¿›å±•è´¨é‡"""
        
        # 1. è¿›å±•è´¨é‡è¯„ä¼°
        progress_quality = await asyncio.gather(
            self._assess_breadth_coverage(current_results, research_brief),
            self._assess_depth_sufficiency(current_results, research_brief),
            self._assess_source_diversity(current_results),
            self._assess_evidence_strength(current_results)
        )
        
        quality_scores = dict(zip(self.completeness_indicators.keys(), progress_quality))
        
        # 2. è¿›å±•è¶‹åŠ¿åˆ†æ
        if len(self.progress_checkpoints) > 0:
            trend_analysis = self._analyze_quality_trends(quality_scores)
        else:
            trend_analysis = {"trend": "initial", "improvement_rate": 0}
        
        # 3. å®Œæ•´æ€§è¯„ä¼°
        completeness_assessment = await self._assess_research_completeness(quality_scores, research_brief)
        
        # 4. è´¨é‡æ£€æŸ¥ç‚¹è®°å½•
        checkpoint = {
            "iteration": iteration,
            "timestamp": time.time(),
            "quality_scores": quality_scores,
            "completeness": completeness_assessment["completeness_score"],
            "total_results": len(current_results)
        }
        self.progress_checkpoints.append(checkpoint)
        
        # 5. å†³ç­–å»ºè®®
        decision_recommendation = await self._generate_progress_decision(
            quality_scores, completeness_assessment, trend_analysis
        )
        
        return {
            "quality_scores": quality_scores,
            "trend_analysis": trend_analysis,
            "completeness_assessment": completeness_assessment,
            "decision_recommendation": decision_recommendation,
            "quality_alerts": self._check_progress_quality_alerts(quality_scores)
        }
    
    async def _assess_breadth_coverage(self, results: List[str], brief: str) -> float:
        """è¯„ä¼°ç ”ç©¶å¹¿åº¦è¦†ç›–"""
        
        # æå–ç ”ç©¶åº”è¯¥è¦†ç›–çš„å…³é”®ç»´åº¦
        expected_dimensions = await self._extract_research_dimensions(brief)
        
        # åˆ†æå½“å‰ç»“æœè¦†ç›–çš„ç»´åº¦
        covered_dimensions = set()
        for result in results:
            result_dimensions = await self._extract_covered_dimensions(result)
            covered_dimensions.update(result_dimensions)
        
        # è®¡ç®—è¦†ç›–åº¦
        if not expected_dimensions:
            return 0.8  # é»˜è®¤è¾ƒå¥½è¦†ç›–
        
        coverage_ratio = len(covered_dimensions & expected_dimensions) / len(expected_dimensions)
        return min(1.0, coverage_ratio)
    
    async def _assess_depth_sufficiency(self, results: List[str], brief: str) -> float:
        """è¯„ä¼°ç ”ç©¶æ·±åº¦å……åˆ†æ€§"""
        
        depth_indicators = {
            "detailed_analysis": 0,      # è¯¦ç»†åˆ†æ
            "multiple_perspectives": 0,   # å¤šè§’åº¦è§†è§’
            "evidence_support": 0,       # è¯æ®æ”¯æ’‘
            "expert_opinions": 0,        # ä¸“å®¶è§‚ç‚¹
            "quantitative_data": 0       # é‡åŒ–æ•°æ®
        }
        
        total_content = "\n".join(results)
        
        # ç®€åŒ–çš„æ·±åº¦è¯„ä¼°ï¼ˆå®é™…åº”è¯¥ä½¿ç”¨æ›´å¤æ‚çš„NLPåˆ†æï¼‰
        if "åˆ†æ" in total_content or "analysis" in total_content.lower():
            depth_indicators["detailed_analysis"] = 1
        
        if "è§‚ç‚¹" in total_content or "perspective" in total_content.lower():
            depth_indicators["multiple_perspectives"] = 1
        
        if "æ•°æ®" in total_content or "ç ”ç©¶" in total_content:
            depth_indicators["evidence_support"] = 1
        
        # è®¡ç®—æ·±åº¦åˆ†æ•°
        depth_score = sum(depth_indicators.values()) / len(depth_indicators)
        return depth_score
    
    def _analyze_quality_trends(self, current_scores: dict) -> dict:
        """åˆ†æè´¨é‡è¶‹åŠ¿"""
        if len(self.progress_checkpoints) < 2:
            return {"trend": "insufficient_data", "improvement_rate": 0}
        
        # è®¡ç®—æœ€è¿‘å‡ æ¬¡çš„è´¨é‡æ”¹è¿›
        recent_checkpoints = self.progress_checkpoints[-3:]  # æœ€è¿‘3æ¬¡
        
        quality_changes = []
        for i in range(1, len(recent_checkpoints)):
            prev_scores = recent_checkpoints[i-1]["quality_scores"]
            curr_scores = recent_checkpoints[i]["quality_scores"]
            
            avg_change = sum(
                curr_scores[key] - prev_scores.get(key, 0)
                for key in curr_scores.keys()
            ) / len(curr_scores)
            
            quality_changes.append(avg_change)
        
        # è¶‹åŠ¿åˆ†æ
        if not quality_changes:
            return {"trend": "stable", "improvement_rate": 0}
        
        avg_improvement = sum(quality_changes) / len(quality_changes)
        
        if avg_improvement > 0.05:
            trend = "improving"
        elif avg_improvement < -0.05:
            trend = "declining"
        else:
            trend = "stable"
        
        return {
            "trend": trend,
            "improvement_rate": avg_improvement,
            "consistency": self._calculate_trend_consistency(quality_changes)
        }
```

## ğŸ“ è¾“å‡ºè´¨é‡è¯„ä¼°

### 1. æœ€ç»ˆæŠ¥å‘Šè´¨é‡è¯„ä¼°

#### 1.1 å¤šç»´åº¦è´¨é‡è¯„ä¼°æ¡†æ¶

```python
class FinalReportQualityAssessor:
    """æœ€ç»ˆæŠ¥å‘Šè´¨é‡è¯„ä¼°å™¨"""
    
    def __init__(self, llm_model):
        self.llm_model = llm_model
        self.quality_dimensions = {
            "content_accuracy": {
                "weight": 0.25,
                "threshold": 0.85,
                "description": "å†…å®¹å‡†ç¡®æ€§å’Œäº‹å®æ­£ç¡®æ€§"
            },
            "structural_coherence": {
                "weight": 0.20,
                "threshold": 0.80,
                "description": "ç»“æ„è¿è´¯æ€§å’Œé€»è¾‘æ€§"
            },
            "completeness": {
                "weight": 0.25,
                "threshold": 0.85,
                "description": "ä¿¡æ¯å®Œæ•´æ€§å’Œè¦†ç›–åº¦"
            },
            "source_credibility": {
                "weight": 0.15,
                "threshold": 0.75,
                "description": "ä¿¡æ¯æ¥æºå¯ä¿¡åº¦"
            },
            "actionability": {
                "weight": 0.15,
                "threshold": 0.70,
                "description": "å®ç”¨æ€§å’Œå¯æ“ä½œæ€§"
            }
        }
    
    async def assess_final_report_quality(self, 
                                        report: str, 
                                        research_brief: str, 
                                        source_materials: List[str]) -> dict:
        """å…¨é¢è¯„ä¼°æœ€ç»ˆæŠ¥å‘Šè´¨é‡"""
        
        # 1. å¹¶è¡Œæ‰§è¡Œå¤šç»´åº¦è¯„ä¼°
        assessment_tasks = [
            self._assess_content_accuracy(report, source_materials),
            self._assess_structural_coherence(report),
            self._assess_completeness(report, research_brief),
            self._assess_source_credibility(report, source_materials),
            self._assess_actionability(report, research_brief)
        ]
        
        dimension_scores = await asyncio.gather(*assessment_tasks)
        
        # 2. æ„å»ºè¯„ä¼°ç»“æœ
        quality_assessment = {}
        for i, (dimension, config) in enumerate(self.quality_dimensions.items()):
            score = dimension_scores[i]
            quality_assessment[dimension] = {
                "score": score,
                "threshold": config["threshold"],
                "passes": score >= config["threshold"],
                "weight": config["weight"]
            }
        
        # 3. è®¡ç®—åŠ æƒæ€»åˆ†
        weighted_score = sum(
            assessment["score"] * assessment["weight"]
            for assessment in quality_assessment.values()
        )
        
        # 4. ç”Ÿæˆè´¨é‡æŠ¥å‘Š
        quality_report = {
            "overall_score": weighted_score,
            "dimension_scores": quality_assessment,
            "passes_threshold": weighted_score >= 0.80,  # æ€»ä½“é˜ˆå€¼
            "quality_grade": self._calculate_quality_grade(weighted_score),
            "improvement_areas": [
                dim for dim, assessment in quality_assessment.items()
                if not assessment["passes"]
            ],
            "strengths": [
                dim for dim, assessment in quality_assessment.items()
                if assessment["score"] >= assessment["threshold"] + 0.1
            ]
        }
        
        # 5. ç”Ÿæˆè¯¦ç»†åˆ†æå’Œå»ºè®®
        detailed_analysis = await self._generate_detailed_quality_analysis(
            report, quality_assessment, research_brief
        )
        
        return {
            "quality_report": quality_report,
            "detailed_analysis": detailed_analysis,
            "recommendations": await self._generate_quality_improvement_recommendations(quality_report)
        }
    
    async def _assess_content_accuracy(self, report: str, source_materials: List[str]) -> float:
        """è¯„ä¼°å†…å®¹å‡†ç¡®æ€§"""
        accuracy_prompt = f"""
        è¯„ä¼°ç ”ç©¶æŠ¥å‘Šçš„å†…å®¹å‡†ç¡®æ€§(0-1)ï¼š
        
        æŠ¥å‘Šå†…å®¹: {report[:2000]}...
        
        æ¥æºææ–™: {chr(10).join(source[:500] for source in source_materials[:3])}...
        
        è¯„ä¼°æ ‡å‡†:
        - äº‹å®é™ˆè¿°æ˜¯å¦å‡†ç¡®
        - æ•°æ®å¼•ç”¨æ˜¯å¦æ­£ç¡®
        - ç»“è®ºæ˜¯å¦åŸºäºè¯æ®
        - æ˜¯å¦å­˜åœ¨æ˜æ˜¾é”™è¯¯
        
        å‡†ç¡®æ€§è¯„åˆ†(0-1): """
        
        response = await self.llm_model.ainvoke([HumanMessage(content=accuracy_prompt)])
        return self._parse_score(response.content)
    
    async def _assess_structural_coherence(self, report: str) -> float:
        """è¯„ä¼°ç»“æ„è¿è´¯æ€§"""
        coherence_prompt = f"""
        è¯„ä¼°ç ”ç©¶æŠ¥å‘Šçš„ç»“æ„è¿è´¯æ€§(0-1)ï¼š
        
        æŠ¥å‘Šå†…å®¹: {report}
        
        è¯„ä¼°æ ‡å‡†:
        - é€»è¾‘ç»“æ„æ˜¯å¦æ¸…æ™°
        - æ®µè½é—´è¿‡æ¸¡æ˜¯å¦è‡ªç„¶
        - è®ºè¯æ˜¯å¦è¿è´¯
        - ç»“è®ºæ˜¯å¦ä¸å†…å®¹ä¸€è‡´
        
        è¿è´¯æ€§è¯„åˆ†(0-1): """
        
        response = await self.llm_model.ainvoke([HumanMessage(content=coherence_prompt)])
        return self._parse_score(response.content)
    
    def _calculate_quality_grade(self, weighted_score: float) -> str:
        """è®¡ç®—è´¨é‡ç­‰çº§"""
        if weighted_score >= 0.95:
            return "ä¼˜ç§€ (A+)"
        elif weighted_score >= 0.90:
            return "ä¼˜ç§€ (A)"
        elif weighted_score >= 0.85:
            return "è‰¯å¥½ (B+)"
        elif weighted_score >= 0.80:
            return "è‰¯å¥½ (B)"
        elif weighted_score >= 0.75:
            return "ä¸­ç­‰ (C+)"
        elif weighted_score >= 0.70:
            return "ä¸­ç­‰ (C)"
        else:
            return "éœ€è¦æ”¹è¿› (D)"
```

#### 1.2 è‡ªåŠ¨åŒ–è´¨é‡æ”¹è¿›å»ºè®®

```python
class QualityImprovementRecommendationEngine:
    """è´¨é‡æ”¹è¿›å»ºè®®å¼•æ“"""
    
    def __init__(self, llm_model):
        self.llm_model = llm_model
        self.improvement_strategies = {
            "content_accuracy": [
                "å¢åŠ äº‹å®æ ¸æŸ¥æ­¥éª¤",
                "å¼•ç”¨æ›´æƒå¨çš„ä¿¡æ¯æº",
                "æ·»åŠ æ•°æ®éªŒè¯æœºåˆ¶",
                "è¿›è¡Œäº¤å‰éªŒè¯"
            ],
            "structural_coherence": [
                "é‡æ–°ç»„ç»‡é€»è¾‘ç»“æ„",
                "æ·»åŠ æ®µè½é—´çš„è¿‡æ¸¡å¥",
                "ç»Ÿä¸€è®ºè¯é£æ ¼",
                "æ˜ç¡®ç»“è®ºä¸è¯æ®çš„å…³ç³»"
            ],
            "completeness": [
                "è¡¥å……é—æ¼çš„å…³é”®ä¿¡æ¯",
                "æ‰©å±•åˆ†ææ·±åº¦",
                "å¢åŠ å¤šè§’åº¦è§†è§’",
                "æ·»åŠ å®ä¾‹å’Œæ¡ˆä¾‹"
            ],
            "source_credibility": [
                "ä½¿ç”¨æ›´æƒå¨çš„ä¿¡æ¯æº",
                "å¢åŠ æ¥æºå¤šæ ·æ€§",
                "æ·»åŠ æ¥æºéªŒè¯",
                "å¼•ç”¨æœ€æ–°èµ„æ–™"
            ],
            "actionability": [
                "æä¾›å…·ä½“çš„è¡ŒåŠ¨å»ºè®®",
                "æ·»åŠ å®æ–½æ­¥éª¤",
                "åŒ…å«å¯é‡åŒ–çš„æŒ‡æ ‡",
                "ç»™å‡ºæ—¶é—´çº¿è§„åˆ’"
            ]
        }
    
    async def generate_improvement_recommendations(self, quality_report: dict, report_content: str) -> dict:
        """ç”Ÿæˆè´¨é‡æ”¹è¿›å»ºè®®"""
        
        improvement_areas = quality_report["improvement_areas"]
        
        if not improvement_areas:
            return {
                "status": "no_improvement_needed",
                "message": "æŠ¥å‘Šè´¨é‡å·²è¾¾åˆ°æ ‡å‡†ï¼Œæ— éœ€æ”¹è¿›"
            }
        
        # 1. é’ˆå¯¹æ¯ä¸ªé—®é¢˜ç»´åº¦ç”Ÿæˆå…·ä½“å»ºè®®
        dimension_recommendations = {}
        for dimension in improvement_areas:
            current_score = quality_report["dimension_scores"][dimension]["score"]
            target_score = quality_report["dimension_scores"][dimension]["threshold"]
            
            specific_recommendations = await self._generate_specific_recommendations(
                dimension, current_score, target_score, report_content
            )
            
            dimension_recommendations[dimension] = {
                "current_score": current_score,
                "target_score": target_score,
                "improvement_gap": target_score - current_score,
                "specific_recommendations": specific_recommendations,
                "priority": self._calculate_improvement_priority(dimension, current_score, target_score)
            }
        
        # 2. ç”Ÿæˆæ”¹è¿›è®¡åˆ’
        improvement_plan = await self._create_improvement_plan(dimension_recommendations)
        
        # 3. ä¼°ç®—æ”¹è¿›æ•ˆæœ
        expected_improvement = self._estimate_improvement_impact(dimension_recommendations, quality_report)
        
        return {
            "improvement_areas": dimension_recommendations,
            "improvement_plan": improvement_plan,
            "expected_improvement": expected_improvement,
            "implementation_timeline": self._create_implementation_timeline(improvement_plan)
        }
    
    async def _generate_specific_recommendations(self, 
                                               dimension: str, 
                                               current_score: float, 
                                               target_score: float, 
                                               report_content: str) -> List[str]:
        """ä¸ºç‰¹å®šç»´åº¦ç”Ÿæˆå…·ä½“æ”¹è¿›å»ºè®®"""
        
        base_strategies = self.improvement_strategies.get(dimension, [])
        
        # ä½¿ç”¨LLMç”Ÿæˆé’ˆå¯¹æ€§å»ºè®®
        recommendation_prompt = f"""
        ä¸ºæé«˜ç ”ç©¶æŠ¥å‘Šçš„{dimension}è´¨é‡ç”Ÿæˆå…·ä½“æ”¹è¿›å»ºè®®ï¼š
        
        å½“å‰è¯„åˆ†: {current_score:.2f}
        ç›®æ ‡è¯„åˆ†: {target_score:.2f}
        æ”¹è¿›ç¼ºå£: {target_score - current_score:.2f}
        
        æŠ¥å‘Šå†…å®¹ç‰‡æ®µ: {report_content[:1000]}...
        
        åŸºç¡€ç­–ç•¥: {', '.join(base_strategies)}
        
        è¯·ç”Ÿæˆ3-5ä¸ªå…·ä½“çš„ã€å¯æ“ä½œçš„æ”¹è¿›å»ºè®®ï¼š
        """
        
        response = await self.llm_model.ainvoke([HumanMessage(content=recommendation_prompt)])
        
        # è§£æLLMå“åº”ä¸ºå»ºè®®åˆ—è¡¨
        recommendations = self._parse_recommendations(response.content)
        
        return recommendations
    
    def _calculate_improvement_priority(self, dimension: str, current_score: float, target_score: float) -> str:
        """è®¡ç®—æ”¹è¿›ä¼˜å…ˆçº§"""
        improvement_gap = target_score - current_score
        dimension_weight = self.quality_dimensions.get(dimension, {}).get("weight", 0.2)
        
        priority_score = improvement_gap * dimension_weight
        
        if priority_score > 0.05:
            return "é«˜"
        elif priority_score > 0.02:
            return "ä¸­"
        else:
            return "ä½"
    
    async def _create_improvement_plan(self, dimension_recommendations: dict) -> dict:
        """åˆ›å»ºæ”¹è¿›è®¡åˆ’"""
        
        # æŒ‰ä¼˜å…ˆçº§æ’åºç»´åº¦
        sorted_dimensions = sorted(
            dimension_recommendations.items(),
            key=lambda x: {"é«˜": 3, "ä¸­": 2, "ä½": 1}[x[1]["priority"]],
            reverse=True
        )
        
        improvement_phases = []
        current_phase = []
        
        for dimension, recommendation in sorted_dimensions:
            if recommendation["priority"] == "é«˜":
                current_phase.append({
                    "dimension": dimension,
                    "actions": recommendation["specific_recommendations"][:2],  # å‰2ä¸ªæœ€é‡è¦çš„è¡ŒåŠ¨
                    "expected_time": "1-2å¤©"
                })
            elif recommendation["priority"] == "ä¸­" and len(current_phase) < 3:
                current_phase.append({
                    "dimension": dimension,
                    "actions": recommendation["specific_recommendations"][:1],  # å‰1ä¸ªè¡ŒåŠ¨
                    "expected_time": "åŠå¤©"
                })
            
            # æ¯3ä¸ªç»´åº¦ç»„æˆä¸€ä¸ªé˜¶æ®µ
            if len(current_phase) >= 3:
                improvement_phases.append(current_phase)
                current_phase = []
        
        # å¤„ç†å‰©ä½™çš„ç»´åº¦
        if current_phase:
            improvement_phases.append(current_phase)
        
        return {
            "phases": improvement_phases,
            "total_phases": len(improvement_phases),
            "estimated_total_time": f"{len(improvement_phases) * 2}-{len(improvement_phases) * 3}å¤©"
        }
```

## ğŸ”„ æŒç»­è´¨é‡æ”¹è¿›æœºåˆ¶

### 1. è´¨é‡åé¦ˆå¾ªç¯

```python
class QualityFeedbackLoop:
    """è´¨é‡åé¦ˆå¾ªç¯ç³»ç»Ÿ"""
    
    def __init__(self):
        self.quality_history = []
        self.improvement_tracking = {}
        self.quality_patterns = {}
    
    async def collect_quality_feedback(self, 
                                     session_id: str,
                                     quality_metrics: dict,
                                     user_feedback: Optional[dict] = None) -> dict:
        """æ”¶é›†è´¨é‡åé¦ˆ"""
        
        feedback_entry = {
            "session_id": session_id,
            "timestamp": time.time(),
            "quality_metrics": quality_metrics,
            "user_feedback": user_feedback,
            "system_performance": await self._collect_system_metrics()
        }
        
        self.quality_history.append(feedback_entry)
        
        # åˆ†æè´¨é‡è¶‹åŠ¿
        quality_trends = await self._analyze_quality_trends()
        
        # è¯†åˆ«æ”¹è¿›æœºä¼š
        improvement_opportunities = await self._identify_improvement_opportunities(feedback_entry)
        
        # æ›´æ–°è´¨é‡æ¨¡å¼
        await self._update_quality_patterns(feedback_entry)
        
        return {
            "feedback_recorded": True,
            "quality_trends": quality_trends,
            "improvement_opportunities": improvement_opportunities,
            "recommendations": await self._generate_system_recommendations(quality_trends)
        }
    
    async def _analyze_quality_trends(self) -> dict:
        """åˆ†æè´¨é‡è¶‹åŠ¿"""
        if len(self.quality_history) < 5:
            return {"trend": "insufficient_data"}
        
        recent_sessions = self.quality_history[-10:]  # æœ€è¿‘10ä¸ªä¼šè¯
        
        # è®¡ç®—å„ç»´åº¦çš„è¶‹åŠ¿
        trend_analysis = {}
        
        for dimension in ["overall_score", "content_accuracy", "completeness", "actionability"]:
            scores = [
                session["quality_metrics"].get(dimension, 0.5)
                for session in recent_sessions
                if dimension in session["quality_metrics"]
            ]
            
            if len(scores) >= 3:
                # ç®€å•çº¿æ€§è¶‹åŠ¿åˆ†æ
                trend_slope = self._calculate_trend_slope(scores)
                trend_analysis[dimension] = {
                    "slope": trend_slope,
                    "direction": "improving" if trend_slope > 0.01 else "declining" if trend_slope < -0.01 else "stable",
                    "current_average": sum(scores[-3:]) / 3,  # æœ€è¿‘3æ¬¡å¹³å‡
                    "variance": self._calculate_variance(scores)
                }
        
        return trend_analysis
    
    async def _identify_improvement_opportunities(self, feedback_entry: dict) -> List[dict]:
        """è¯†åˆ«æ”¹è¿›æœºä¼š"""
        opportunities = []
        
        quality_metrics = feedback_entry["quality_metrics"]
        user_feedback = feedback_entry.get("user_feedback", {})
        
        # åŸºäºè´¨é‡æŒ‡æ ‡çš„æ”¹è¿›æœºä¼š
        for dimension, score in quality_metrics.items():
            if isinstance(score, (int, float)) and score < 0.8:
                opportunities.append({
                    "type": "quality_metric",
                    "dimension": dimension,
                    "current_score": score,
                    "improvement_potential": 0.9 - score,
                    "priority": "high" if score < 0.7 else "medium"
                })
        
        # åŸºäºç”¨æˆ·åé¦ˆçš„æ”¹è¿›æœºä¼š
        if user_feedback:
            satisfaction_score = user_feedback.get("satisfaction", 0.8)
            if satisfaction_score < 0.8:
                opportunities.append({
                    "type": "user_satisfaction",
                    "current_score": satisfaction_score,
                    "feedback_details": user_feedback.get("comments", ""),
                    "priority": "high"
                })
        
        # åŸºäºç³»ç»Ÿæ€§èƒ½çš„æ”¹è¿›æœºä¼š
        system_metrics = feedback_entry["system_performance"]
        if system_metrics.get("response_time", 0) > 120:  # è¶…è¿‡2åˆ†é’Ÿ
            opportunities.append({
                "type": "performance",
                "metric": "response_time",
                "current_value": system_metrics["response_time"],
                "target_value": 90,
                "priority": "medium"
            })
        
        return sorted(opportunities, key=lambda x: {"high": 3, "medium": 2, "low": 1}[x["priority"]], reverse=True)
```

### 2. è‡ªé€‚åº”è´¨é‡æ ‡å‡†

```python
class AdaptiveQualityStandards:
    """è‡ªé€‚åº”è´¨é‡æ ‡å‡†ç³»ç»Ÿ"""
    
    def __init__(self):
        self.dynamic_thresholds = {
            "content_accuracy": 0.85,
            "completeness": 0.80,
            "coherence": 0.75,
            "actionability": 0.70
        }
        self.context_modifiers = {}
        self.performance_history = []
    
    async def adjust_quality_standards(self, context: dict, historical_performance: List[dict]) -> dict:
        """æ ¹æ®ä¸Šä¸‹æ–‡å’Œå†å²è¡¨ç°è°ƒæ•´è´¨é‡æ ‡å‡†"""
        
        # 1. åˆ†æä¸Šä¸‹æ–‡å› ç´ 
        context_factors = await self._analyze_context_factors(context)
        
        # 2. è¯„ä¼°å†å²è¡¨ç°
        performance_analysis = self._analyze_historical_performance(historical_performance)
        
        # 3. è®¡ç®—è°ƒæ•´å› å­
        adjustment_factors = await self._calculate_adjustment_factors(context_factors, performance_analysis)
        
        # 4. åº”ç”¨è‡ªé€‚åº”è°ƒæ•´
        adjusted_thresholds = {}
        for dimension, base_threshold in self.dynamic_thresholds.items():
            adjustment = adjustment_factors.get(dimension, 0)
            adjusted_threshold = base_threshold + adjustment
            
            # ç¡®ä¿é˜ˆå€¼åœ¨åˆç†èŒƒå›´å†…
            adjusted_threshold = max(0.5, min(0.95, adjusted_threshold))
            adjusted_thresholds[dimension] = adjusted_threshold
        
        # 5. è®°å½•è°ƒæ•´å†å²
        adjustment_record = {
            "timestamp": time.time(),
            "context": context,
            "base_thresholds": self.dynamic_thresholds.copy(),
            "adjusted_thresholds": adjusted_thresholds,
            "adjustment_factors": adjustment_factors
        }
        
        return {
            "adjusted_thresholds": adjusted_thresholds,
            "adjustment_rationale": await self._generate_adjustment_rationale(adjustment_factors, context),
            "expected_impact": self._estimate_adjustment_impact(adjustment_factors),
            "adjustment_record": adjustment_record
        }
    
    async def _analyze_context_factors(self, context: dict) -> dict:
        """åˆ†æå½±å“è´¨é‡æ ‡å‡†çš„ä¸Šä¸‹æ–‡å› ç´ """
        
        factors = {
            "complexity": 0,      # ç ”ç©¶å¤æ‚åº¦
            "urgency": 0,         # ç´§æ€¥ç¨‹åº¦
            "domain": 0,          # é¢†åŸŸä¸“ä¸šæ€§
            "scope": 0,           # ç ”ç©¶èŒƒå›´
            "resources": 0        # å¯ç”¨èµ„æº
        }
        
        # å¤æ‚åº¦è¯„ä¼°
        research_brief = context.get("research_brief", "")
        if len(research_brief.split()) > 100:
            factors["complexity"] += 0.1
        if "æ¯”è¾ƒ" in research_brief or "compare" in research_brief.lower():
            factors["complexity"] += 0.05
        
        # ç´§æ€¥ç¨‹åº¦
        time_constraint = context.get("time_constraint", "normal")
        if time_constraint == "urgent":
            factors["urgency"] = 0.1
        elif time_constraint == "relaxed":
            factors["urgency"] = -0.05
        
        # é¢†åŸŸä¸“ä¸šæ€§
        domain_keywords = ["æŠ€æœ¯", "scientific", "academic", "research"]
        if any(keyword in research_brief.lower() for keyword in domain_keywords):
            factors["domain"] = 0.05
        
        return factors
    
    def _analyze_historical_performance(self, historical_performance: List[dict]) -> dict:
        """åˆ†æå†å²è¡¨ç°æ¨¡å¼"""
        if len(historical_performance) < 5:
            return {"trend": "insufficient_data", "average_performance": 0.8}
        
        recent_performance = historical_performance[-10:]  # æœ€è¿‘10æ¬¡
        
        # è®¡ç®—å„ç»´åº¦çš„å¹³å‡è¡¨ç°
        dimension_averages = {}
        for dimension in self.dynamic_thresholds.keys():
            scores = [
                perf.get(dimension, 0.8) 
                for perf in recent_performance 
                if dimension in perf
            ]
            if scores:
                dimension_averages[dimension] = sum(scores) / len(scores)
        
        # æ•´ä½“è¶‹åŠ¿åˆ†æ
        overall_scores = [
            sum(perf.values()) / len(perf) 
            for perf in recent_performance 
            if perf
        ]
        
        trend_slope = self._calculate_trend_slope(overall_scores) if overall_scores else 0
        
        return {
            "dimension_averages": dimension_averages,
            "overall_trend": trend_slope,
            "performance_stability": self._calculate_variance(overall_scores) if overall_scores else 0.1
        }
```

## ğŸ¯ é¢è¯•è¦ç‚¹æ€»ç»“

### æ ¸å¿ƒæŠ€æœ¯æ¦‚å¿µ

1. **å¤šç»´åº¦è´¨é‡è¯„ä¼°**: ä»è¾“å…¥åˆ°è¾“å‡ºçš„å…¨æµç¨‹è´¨é‡æ§åˆ¶
2. **å®æ—¶è´¨é‡ç›‘æ§**: æ‰§è¡Œè¿‡ç¨‹ä¸­çš„è´¨é‡è¿½è¸ªå’Œé¢„è­¦
3. **è‡ªé€‚åº”è´¨é‡æ ‡å‡†**: åŸºäºä¸Šä¸‹æ–‡å’Œå†å²è¡¨ç°çš„åŠ¨æ€è°ƒæ•´
4. **è´¨é‡åé¦ˆå¾ªç¯**: æŒç»­æ”¹è¿›çš„é—­ç¯è´¨é‡ç®¡ç†

### ç³»ç»Ÿè®¾è®¡èƒ½åŠ›å±•ç¤º

1. **è´¨é‡æ¶æ„è®¾è®¡**: åˆ†å±‚çš„è´¨é‡ä¿è¯ä½“ç³»
2. **ç›‘æ§ä½“ç³»**: å®æ—¶è´¨é‡ç›‘æ§å’Œé¢„è­¦æœºåˆ¶
3. **è¯„ä¼°æ¡†æ¶**: å¤šç»´åº¦ã€å¯é‡åŒ–çš„è´¨é‡è¯„ä¼°
4. **æ”¹è¿›æœºåˆ¶**: è‡ªåŠ¨åŒ–çš„è´¨é‡æ”¹è¿›å»ºè®®å’Œå®æ–½

### æŠ€æœ¯æ·±åº¦è®¨è®º

1. **è´¨é‡å»ºæ¨¡**: å¦‚ä½•å°†ä¸»è§‚è´¨é‡è½¬åŒ–ä¸ºå®¢è§‚æŒ‡æ ‡
2. **å®æ—¶è¯„ä¼°**: å¤§è§„æ¨¡ç³»ç»Ÿä¸­çš„æ€§èƒ½ä¼˜åŒ–è´¨é‡ç›‘æ§
3. **è‡ªé€‚åº”æœºåˆ¶**: åŠ¨æ€è°ƒæ•´è´¨é‡æ ‡å‡†çš„ç®—æ³•è®¾è®¡
4. **äººæœºç»“åˆ**: AIè¯„ä¼°ä¸äººå·¥åé¦ˆçš„æœ‰æ•ˆç»“åˆ

### å®é™…åº”ç”¨ä»·å€¼

1. **è´¨é‡ä¿è¯**: ç¡®ä¿AIç³»ç»Ÿè¾“å‡ºçš„å¯é æ€§
2. **æŒç»­æ”¹è¿›**: é€šè¿‡åé¦ˆå¾ªç¯ä¸æ–­æå‡ç³»ç»Ÿè´¨é‡
3. **é£é™©æ§åˆ¶**: åŠæ—¶å‘ç°å’Œå¤„ç†è´¨é‡é—®é¢˜
4. **ç”¨æˆ·æ»¡æ„åº¦**: é€šè¿‡è´¨é‡æ§åˆ¶æå‡ç”¨æˆ·ä½“éªŒ

---

è¿™ç§å…¨é¢çš„è´¨é‡æ§åˆ¶ä¸è¯„ä¼°ä½“ç³»ä½“ç°äº†å¯ä¿¡AIç³»ç»Ÿçš„æ ¸å¿ƒè¦æ±‚ï¼Œé€šè¿‡å¤šå±‚æ¬¡çš„è´¨é‡ä¿è¯æœºåˆ¶ç¡®ä¿äº†ç ”ç©¶ç»“æœçš„å‡†ç¡®æ€§å’Œå¯é æ€§ã€‚ 